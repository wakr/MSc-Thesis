\subsection{Source-code plagiarism}

Source-code plagiarism refers to a plagiarism that happens between source code files. This 
scenario is common in academic programming courses and in software industry, where detection can be impossible due to time constraints. In academia, the underlying motives behind source code plagiarism include things like \cite{DPPA2008}; ambiguity about what is considered as excessive collaboration between students, using other students work to gain grades and minimizing the work needed to complete the course. Also, some students have problems to define what they consider as source-code plagiarism, and thus three common guidelines can be given \cite{Pieterse2014DecodingCP}:

\begin{enumerate}
    \item[1)] Refactoring other students work, and submitting it as your own, is plagiarism
    \item[2)] If exercise templates are used, possible similarities between documents and templates are not plagiarism
    \item[3)] Submitting a direct copy of other students work is plagiarism
\end{enumerate}

\noindent
%Because of possible confusion about what is considered as plagiarism and the serious nature of plagiarism accusations in academia, it's preferable still to use some kind of human expert to judge to call out if a student has actually plagiarized someone \cite{Pieterse2014DecodingCP}. This allows the detection process to reveal candidates, which prunes heavily the amount of manual work needed.
Detecting 2. and 3. are straightforward; code templates can be filtered out from documents so that they contain only students own work and detecting direct copy can be easily found by using string matching techniques. However, the problem arises when students tries to hide the plagiarism by mutating the copied document.

\paragraph{Plagiarism strategies}\mbox{}\\
Some common source code transformation techniques, often called as \emph{obfuscation strategies}, are targeted mainly towards two types of changes \cite{DPPA2008}: lexical and structural. Lexical changes doesn't require a deeper understanding of the logic and are doable with any \emph{integrated development environment} (IDE). Structural changes requires some understanding of the program logic, and includes modifications which change the layout of the source code but keeps the logic same. For example considering clause with an operand \texttt{if(a == true)}, this can be written equally as \texttt{if(a == !false)}.

\begin{table}[ht]
\centering
\caption{Common transformation targets.}
\label{tbl-plag-strat}
\begin{tabular}{|l|l|} \hline
 \textbf{Lexical} & \textbf{Structural} \\ \hhline{|=|=|}
 Comments                    & Loops                          \\
 Formatting                  & Clauses                        \\
 Naming                      & Statement order                \\
                             & Operand order               \\ \hline
\end{tabular}
\end{table}

Table \ref{tbl-plag-strat} shows some of the most common transformation targets in source codes. Given a source code from another student, plagiarist can apply above transformations and complicate the task for a human to spot plagiarism, or even confuse naïve methods. The motivation behind using these transformations is simple, plagiarists want to hide traces and thus, the detection method must be resilient against these strategies.


Plagiarism transformations defined in table \ref{tbl-plag-strat} closely relate to an earlier study which characterized six levels of transformations \cite{Faidhi:1987:EAD:27319.27321}.

\begin{table}[ht]
\centering
\caption{Transformation levels.}
\label{tbl-plag-transf}
\scalebox{0.85}{
    \begin{tabular}{|c||c|p{5cm}|} \hline
     \textbf{Level of change} & \textbf{Target}  & \textbf{Example action}\\ \hhline{|=|=|=|}
     1 & Comments and indentation & Add extra spaces and newlines\\ \hline
     2 & Identifiers & Rename all variables\\ \hline
     3 & Declarations & Reorder functions\\ \hline
     4 & Modules & Merge functions\\ \hline
     5 & Statements & Use \texttt{for} instead of \texttt{while}\\ \hline
     6 & Logic & Change whole expressions\\ \hline
    
    \end{tabular}
    }
\end{table}

\noindent
Applying all of these transformations one after another, makes the detection of plagiarism very difficult, as the plagiarized document diverges too much from the original document and hides most of the traces that could be used for detection. However, as the textual information changes, plagiarists still try to maintain the same logic between original and copied documents. This implies that there still exists some kind of similarity, but now the similarity can't be found directly from the textual representation of a source code. Information about the logical structure is thus crucial and accessible when source code is parsed to a tree format.

\paragraph{Code structure}\mbox{}\\
Source code is a structured text, made of keywords and user-defined variables. To write a running program, one must know the rules \ie the \emph{grammar} of a language, which is usually represented as the order in which various keywords and variables must follow each others. A compiler is the core of a programming language and is able to transform a source code into a machine code. When rules must be interpreted by the compiler, it uses a \emph{parse tree} that is generated from the source code \cite{johnson1975yacc}. This parse tree captures the syntax and semantics of a source and the abstracted version of it, is called \emph{abstract syntax tree}. Consider for example storing an integer value to a variable, the source code and its syntax tree is visible from the following diagram.

\begin{diagram}[ht]
\centering
\scalebox{1}{
\Tree[.VariableDeclaration [.Identifier "a" ] [.Literal "5" ] ]
}
\caption{Example syntax tree for \texttt{var a = 5;}}
\label{diag-parse}
\end{diagram}

\noindent
Pruning the leaves of diagram \ref{diag-parse} leads to a more general expression that captures the logic of the source code, becoming resistant against most of the transformations given in table \ref{tbl-plag-transf}. This gives ability to detect similar \emph{structure} rather than similar \emph{tokens}, which are more vulnerable to simple transformations.

\paragraph{Tools}\mbox{}\\
Because plagiarism is kept as a serious offence in academia, a lot of various detection software has been made for it. Novak lists seven of the most well-known tools in a review and explains some common properties \cite{RSCAD2016}: \emph{MOSS, JPlag, SPLaT, SIM, Marble, Plaggie} and \emph{Sherlock}. These tools can be classified into five different categories based on methods used: text, token, graph, tree and hybrid. Among these tools, the most common way to detect plagiarism is a five step approach: pre-process documents, tokenize documents, exclude templates, calculate similarities and finding suspects using these similarity scores.

\newpage

\subsection{Similarity detection} \label{chap-sd}

Similarity detection, or code clone detection, focuses directly on finding similar functionality between arbitrary pair of source codes $\{d_i, d_j\}$. We define it formally as following

\newtheorem*{smd1}{Similarity detection}

\begin{smd1}
Given a set of source code documents $D = \{d_1,...,d_n\}$ called corpus, define normalized similarity function $sim: d_i, d_j \rightarrow [0, 1]$ where $1 \leq i, j \leq n$, such that $sim(d_i, d_j) = sim(d_j, d_i)$ and $sim(d_i, d_i) = 1$, with a optional threshold $\theta \in [0, 1]$ that defines the limit where two source codes are considered as too similar. With this definition, any pair of source code file $(d_i, d_j) \in D \times D$ can also be presented as a triplet $(d_i, d_j, s)$, where $s$ is the similarity value between documents. 
\end{smd1}

\noindent
Above definition is flexible enough to support multiple solutions for the same task, the only restriction being that one must define a function that is able to return a score that either captures the textual or functional similarity. Defined method should be as resilient as possible against transformations given in table \ref{tbl-plag-strat} and table \ref{tbl-plag-transf}, so that two logically similar programs will get a high score even if their textual information differs. Scoring here refers to the value given by similarity function, which can be thought as a distance between documents.

The approach one takes, is ultimately based on how source code document is seen as a data \cite{Roy:2009:CEC:1530898.1531101}: document consisting of plain text, series of tokens, syntax tree, series of metrics or as a graph. The textual approach is based on detecting fragments of copy and paste, and requires no pre-processing if one uses naïve methods like string matching. Another way of approaching is by selecting a specific analysis method: attribute-counting-metrics, which are high-level metrics or by structure-metrics which are able to capture more low level representation of the source code \cite{Verco:1996:SDS:369585.369598}. Despite the approach, some kind of pre-processing and normalization is practically always used to prevent the method to be confused with common obfuscation strategies. 

The core process to detect similarity can be visualized as in figure \ref{fig-sd-flow}, which follows the general structure seen in many state of the art systems \cite{Roy:2009:CEC:1530898.1531101}.

\begin{figure}[!ht]
\centering
\vspace{0.5cm}
\scalebox{0.75}{
   \begin{tikzpicture}[node distance=2cm, baseline]

    \node (start) [io] {Corpus};
    \node (pro1) [process, right of=start, xshift=4.1cm] {Pre-process};
    \node (pro2) [process, right of=pro1, xshift=5cm] {Transform};
    \node (pro3) [process, below of=pro2] {Clone detection};
    \node (end) [io, below of=start] {Suspects};
    
    \draw [arrow] (start) --  node[anchor=south] {Raw source code} (pro1);
    \draw [arrow] (pro1) -- node[anchor=south] {Partition} (pro2);
    \draw [arrow] (pro2) -- node[anchor=east] {Intermediate representation} (pro3);
    \draw [arrow] (pro3) -- node[anchor=north] {Pairwise similarity} (end);
    
    \end{tikzpicture}
}
\caption{Similarity detection process}
\label{fig-sd-flow}
\end{figure}

\noindent
In figure \ref{fig-sd-flow} after the corpus has been set, pre-process stage takes as an input the unmodified source codes to perform two key tasks: to remove unnecessary segments and to determine the level of comparison granularity. The granularity one chooses can range from function-level to document-level, depending how accurately the results should pinpoint code reuse. After the source code has been partitioned, it is transformed into intermediate representation which consists of two parts: extraction that transforms the data to be used for clone detection algorithm \eg by parsing, tokenizing or building a control flow; and normalization. In latter one can apply techniques which reduce the noise between source codes: comments and whitespace removal, uniforming user-defined identifiers and/or removing syntactic noise.

The most key issue regarding to similarity detection, are false-positives that can be handled by manual verification after the suspects are gathered \cite{Verco:1996:SDS:369585.369598, Roy:2009:CEC:1530898.1531101}. This is often mandatory as detection algorithms can't completely understand nuances related to actual plagiarism \eg what is considered as being too similar. 


%\begin{minipage}{.45\hsize}
%\begin{lstlisting}[language=Java, caption=Example code 1]{asd1}
%void code()
%{
%
%}
%\end{lstlisting}
%\end{minipage}\hfill
%\begin{minipage}{.45\hsize}
%
%\begin{lstlisting}[language=Java, caption=Example code 2]{asd2}
%void code()
%{
%
%}
%\end{lstlisting}
%\end{minipage}


\subsection{Authorship identification} \label{chap-ai}

Authorship identification deals with the issue of trying to name the author of a document given some previous work of the author. This problem can be seen as a classification task \cite{KRSUL1997233}, thus we can define authorship identification formally as following.

\newtheorem*{aui1}{Authorship identification}
\begin{aui1}
Given a set of documents $D$, a set of authors $A$ and a function $f: D \rightarrow A$ that identifies the writer by assigning every source code document $d \in D$ to one author $a \in A$. Estimate $f$ with $\hat{f}$, a classifier that treats every document as a feature vector $\bolditt{x}$ and every known class as a vector $\bolditt{y}$. The predicted author $\hat{y}$ can be thus expressed with $\hat{f}(\bolditt{x}) = \hat{y}$.
\end{aui1}

\noindent
This classifier should be able to discriminate between writing styles of different authors, which in programming refers to the habits \ie programming style. Habits are restricted by the grammar of the chosen programming language, but commonly refers to everything that is controllable by the author \eg how one names variables or uses spacing. As a problem, it's therefore very opposite to the similarity detection where we want to find maximum equivalency between programs. 

One way to represent programming style is by using software metrics \cite{KRSUL1997233}. Software metrics can be put into roughly three categories: layout being fragile metrics which are easily transformed by the IDE, style which are non-fragile metrics related to layout and lastly structure which can capture experience and ability of the programmer. As source code can be also thought as a text written with a specific language, majority of the methods using a natural language can be applied, thus we can form a five-level categorization for stylistic features called \emph{stylometrics features}  \cite{Stamatatos:2009:SMA:1527090.1527102}. Categorization for stylometric features is visualized in table \ref{tbl-ai-stylomet}, where semantic features are the most difficult to form as they require understanding deeper meaning of the written source code.

\begin{table}[ht]
\centering
\caption{Five-levels of stylometrics features. }
\label{tbl-ai-stylomet}
\begin{tabular}{|c|c|} \hline
\textbf{Category}             & \textbf{Feature examples} \\ \hline
Lexical              & Token statistics, word n-grams                   \\
Character            & Character n-grams, types, compression            \\
Syntactic            & Errors, expressions, keywords, parse tree        \\
Semantic             & Synonyms, functional dependencies                \\
Application-specific & Indentation, language-specific \\ \hline
\end{tabular}
\end{table}

In natural language authorship analysis, statistical methods are often being used \cite{Stamatatos:2009:SMA:1527090.1527102}, and more specifically machine learning to find reoccurring patterns that are able to distinguish between writing styles. The training of these statistical models can happen in two ways: via profile-based or via instance-based. In profile-based approach, all documents that are presented as training data for an author  are concatenated into one file. In instance-based learning, each text is used as an individual data point. 

Whichever the choice is, one is able to reduce the problem into multiclass classification \cite{Stamatatos:2009:SMA:1527090.1527102}, where author pool of size $n$ can be encoded into binary vector $\bolditt{y} = [y_1, y_2, \cdots, y_n], y_i \in \{0, 1\}$. With this representation if the $i$th candidate is the actual author, the value of that field will be one and the rest are zeroes. 


\subsection{Information retrieval}

Information retrieval (IR) is a collection of strategies of finding documents from large collections \cite{Manning:2008:IIR:1394399}. These documents are often represented as unstructured text and possible methods covers topic like: clustering documents to find similar groups, classifying documents based on their content, ranking text for query search and building search engines. In this thesis as we mainly focus on finding similarities between documents and how to classify the author, we disregard some of the query-based focus of IR and use techniques that are relevant to plagiarism study \ie how document can be represented for machine learning models and how distance between two documents can be calculated.




\subsubsection{Document representation}

As we need some form of numerical way \ie vector to represent one document, we use following IR-related concepts to express the documents in our corpus: \emph{vector space model} which captures algebraically the representation of the document and a \emph{weighting scheme} which helps to give more importance to specific terms.


\paragraph{Vector space model}
%Tokenization

One form of vector space model is called \emph{a binary term-document incidence matrix} \cite{Manning:2008:IIR:1394399}, which represents documents as columns of the matrix and terms as the rows. Terms are gathered by tokenization procedure which divides single document into units that are often \emph{words} of the document. Let $\bolditt{M}_{n \times k}$ be this matrix having $n$ terms and $k$ documents, then the value of $\bolditt{M}_{d, t}$ \ie term $t$ appearing in document $d$, is 1 if it appears at all and zero otherwise. Table \ref{tbl-binmatr} shows example matrix build from programs in appendix \ref{appendix:programs}.  

\begin{table}[ht]
\centering
\caption{Example of a binary term-document incidence matrix for three sample programs in appendix \ref{appendix:programs}.}
\label{tbl-binmatr}
\begin{tabular}{|c|c|c|c|} \hline
      \backslashbox{\bf Term}{\bf Document} & A & B & C \\ \hline
\texttt{public} & 1 & 1 & 1 \\
\texttt{sum}   & 0 & 1 & 0 \\
\texttt{double} & 1 & 1 & 0 \\
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
\texttt{b} & 1 & 1 & 1 \\ \hline
\end{tabular}
\end{table}

\noindent
Taking a transpose of the matrix in table \ref{tbl-binmatr} gives a document-term matrix, where one row represents now document having $n$ dimensions reserving one dimension per each term occurrence. For example $\bolditt{a} = [1, 0, 1, \cdots, 1]$. This is referred also as the \emph{bag of words} model, because it treats every word as independent event, losing the information about ordering \cite{Manning:2008:IIR:1394399}. Unigrams can be thus expressed as a product of term probabilities 

\begin{equation}
    P(t_1, t_2, t_3, t_4) = P(t_1)P(t_2)P(t_3)P(t_4)
\end{equation}

A binary term-document incidence matrix is however very naïve, giving the same value despite the times term appears in a document. The solution is to use a  method called \emph{term weighting}, which is able to assign non-binary value for terms. 

\paragraph{Term weighting schemes}

One simple scheme is called \emph{term frequency}, which is the occurrence of term $t$ in document $d$ denoted by $tf_{t, d}$ \cite{Manning:2008:IIR:1394399}. Let $f_{t, d}$ denote the raw frequency count, then term frequency can be given as $tf_{t, d} = f_{t, d}$ and normalized by dividing the raw frequency with the total frequency over every term in document

\begin{equation} \label{eq-tf}
    tf_{t, d} = \dfrac{f_{t, d}}{\sum \limits_{t' \in d} f_{t', d}}
\end{equation}

To scale down the most frequently appearing terms, one can use \emph{inverse document frequency} which boosts the weights of rare occurring terms \cite{Manning:2008:IIR:1394399}. Inverse document frequency is defined as

\begin{equation}
    idf_t = \log \dfrac{N}{df_t}
\end{equation}

\noindent
Where $N$ is the total amount of documents and $df_{t}$ is the count of documents that contains the term $t$.

Now by taking the product of term frequency and inverse document frequency (tf-idf), we get a weight for each term appearing in a document. It's defined as

\begin{equation}
    tf{\text -}idf_{t, d} = tf_{t, d} \cdot idf_{t}
\end{equation}

By using tf-idf weighting scheme, we are able to discriminate between documents and diminish the problem of frequently appearing terms that are introduced often in programming, because the language is very structured and defined by a finite amount of keywords and variables. 

\subsubsection{Document similarity}

Now as we have a way of expressing a document as a vector, we need to somehow be able to calculate similarity between two documents, which is crucial for similarity detection. One way of doing this is by using \emph{cosine similarity}.

\paragraph{Cosine similarity}

Cosine similarity measures the similarity between two documents by calculating the cosine of the angle between the vector representations \cite{Manning:2008:IIR:1394399}. Let $\bolditt{x}, \bolditt{y}$ be these vectors for documents $d_1, d_2$, then

\begin{equation} \label{eq-cosine-orig}
    sim(d_1, d_2) = \cos(\theta) = \dfrac{\bolditt{x} \boldsymbol{\cdot} \bolditt{y}}
                          {\norm{\bolditt{x}}_2 \norm{\bolditt{y}}_2} = 
                          \dfrac{\sum \limits_{i=1}^n x_i y_i}
                                {\sqrt{\sum \limits_{i}^n {x}_i^2} \sqrt{\sum \limits_{i}^n y_i^2}}
\end{equation}

\noindent
As the dot-product is normalized in equation \ref{eq-cosine-orig} with Euclidean norm and by using non-negative weights derived from tf-idf, the cosine similarity gets values between zero to one and values closer to one indicate from high content similarity. The complement of cosine similarity called \emph{cosine distance} can be calculated by $d = 1 - \cos(\theta)$.

\subsubsection{Retrieval metrics}

Having a way to retrieve candidate documents for possible plagiarism, creates a need to justify how well the model is performing. For evaluating the retrieval method in a binary case, three important metrics have been defined \cite{Manning:2008:IIR:1394399}: precision, recall and $F_1$-score. To express these metrics we use a confusion matrix which has four fields: true positive (TP), true negative (TN), false negative (FN) and false positive (FP). The meaning of these fields is visible from table \ref{tbl-confmatr-orig}.


\begin{table}[ht]
\centering
\caption{Confusion matrix \cite{Manning:2008:IIR:1394399}.}
\label{tbl-confmatr-orig}
\begin{tabular}{c|c|c}
          & \bf Relevant & \bf Irrelevant \\ \hline
\bf Retrieved & TP      & FP        \\
\bf Rejected  & FN      & TN       
\end{tabular}
\end{table}

Precision and recall can be now defined by using the confusion matrix. Both precision and recall calculate the rate of true positives to falsely retrieved documents, and balancing between these values requires often knowledge about the domain. If precision is preferred to be high, model is able to correctly retrieve greater portion of correct positive cases and if recall is preferred, model is able to retrieve high portion of relevant documents. 

\begin{align}
    \text{Precision } &= \dfrac{TP}{TP + FP} = \dfrac{|\text{relevant} \cap \text{retrieved}|}{|\text{retrieved}|}\\
    \text{Recall } &= \dfrac{TP}{TP + FN} = \dfrac{|\text{relevant} \cap \text{retrieved}|}{|\text{relevant}|}
\end{align}

$F_1$-score is slightly different metric, giving an average between precision and recall. It's defined as

\begin{equation}
    F_1 = 2 \cdot \dfrac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}

\newpage

\subsection{Supervised learning}

Statistical method.

\subsubsection{Naive Bayes}

\subsubsection{Classification}

\paragraph{One vs Rest}



\subsection{Unsupervised learning}

\paragraph{K-means}

\paragraph{DBSCAN}

