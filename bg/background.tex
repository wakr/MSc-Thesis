\subsection{Source-code plagiarism} \label{chap-bg-sc-plag}

Source-code plagiarism refers to a plagiarism that happens between source code files. This 
scenario is common in academic programming courses and in software industry, where detection can be impossible due to time constraints. In academia, the underlying motives behind source code plagiarism include things like \cite{DPPA2008}; ambiguity about what is considered as excessive collaboration between students, using other students work to gain grades and minimizing the work needed to complete the course. Also, some students have problems to define what they consider as source-code plagiarism, and thus three common guidelines can be given \cite{Pieterse2014DecodingCP}:

\begin{enumerate}
    \item[1)] Refactoring other students work, and submitting it as your own, is plagiarism
    \item[2)] If exercise templates are used, possible similarities between documents and templates are not plagiarism
    \item[3)] Submitting a direct copy of other students work is plagiarism
\end{enumerate}

\noindent
%Because of possible confusion about what is considered as plagiarism and the serious nature of plagiarism accusations in academia, it's preferable still to use some kind of human expert to judge to call out if a student has actually plagiarized someone \cite{Pieterse2014DecodingCP}. This allows the detection process to reveal candidates, which prunes heavily the amount of manual work needed.
Detecting 2. and 3. are straightforward; code templates can be filtered out from documents so that they contain only students own work and detecting direct copy can be easily found by using string matching techniques. However, the problem arises when students tries to hide the plagiarism by mutating the copied document.

\paragraph{Plagiarism strategies}\mbox{}\\
Some common source code transformation techniques, often called as \emph{obfuscation strategies}, are targeted mainly towards two types of changes \cite{DPPA2008}: lexical and structural. Lexical changes doesn't require a deeper understanding of the logic and are doable with any \emph{integrated development environment} (IDE). Structural changes requires some understanding of the program logic, and includes modifications which change the layout of the source code but keeps the logic same. For example considering clause with an operand \texttt{if(a == true)}, this can be written equally as \texttt{if(a == !false)}.

\begin{table}[ht]
\centering
\caption{Common transformation targets.}
\label{tbl-plag-strat}
\begin{tabular}{|l|l|} \hline
 \textbf{Lexical} & \textbf{Structural} \\ \hhline{|=|=|}
 Comments                    & Loops                          \\
 Formatting                  & Clauses                        \\
 Naming                      & Statement order                \\
                             & Operand order               \\ \hline
\end{tabular}
\end{table}

Table \ref{tbl-plag-strat} shows some of the most common transformation targets in source codes. Given a source code from another student, plagiarist can apply above transformations and complicate the task for a human to spot plagiarism, or even confuse naïve methods. The motivation behind using these transformations is simple, plagiarists want to hide traces and thus, the detection method must be resilient against these strategies.


Plagiarism transformations defined in table \ref{tbl-plag-strat} closely relate to an earlier study which characterized six levels of transformations \cite{Faidhi:1987:EAD:27319.27321}.

\begin{table}[ht]
\centering
\caption{Transformation levels.}
\label{tbl-plag-transf}
\scalebox{0.85}{
    \begin{tabular}{|c||c|p{5cm}|} \hline
     \textbf{Level of change} & \textbf{Target}  & \textbf{Example action}\\ \hhline{|=|=|=|}
     1 & Comments and indentation & Add extra spaces and newlines\\ \hline
     2 & Identifiers & Rename all variables\\ \hline
     3 & Declarations & Reorder functions\\ \hline
     4 & Modules & Merge functions\\ \hline
     5 & Statements & Use \texttt{for} instead of \texttt{while}\\ \hline
     6 & Logic & Change whole expressions\\ \hline
    
    \end{tabular}
    }
\end{table}

\noindent
Applying all of these transformations one after another, makes the detection of plagiarism very difficult, as the plagiarized document diverges too much from the original document and hides most of the traces that could be used for detection. However, as the textual information changes, plagiarists still try to maintain the same logic between original and copied documents. This implies that there still exists some kind of similarity, but now the similarity can't be found directly from the textual representation of a source code. Information about the logical structure is thus crucial and accessible when source code is parsed to a tree format.

\paragraph{Code structure}\mbox{}\\
Source code is a structured text, made of keywords and user-defined variables. To write a running program, one must know the rules \ie the \emph{grammar} of a language, which is usually represented as the order in which various keywords and variables must follow each others. A compiler is the core of a programming language and is able to transform a source code into a machine code. When rules must be interpreted by the compiler, it uses a \emph{parse tree} that is generated from the source code \cite{johnson1975yacc}. This parse tree captures the syntax and semantics of a source and the abstracted version of it, is called \emph{abstract syntax tree}. Consider for example storing an integer value to a variable, the source code and its syntax tree is visible from the following diagram.

\begin{diagram}[ht]
\centering
\scalebox{1}{
\Tree[.VariableDeclaration [.Identifier "a" ] [.Literal "5" ] ]
}
\caption{Example syntax tree for \texttt{var a = 5;}}
\label{diag-parse}
\end{diagram}

\noindent
Pruning the leaves of diagram \ref{diag-parse} leads to a more general expression that captures the logic of the source code, becoming resistant against most of the transformations given in table \ref{tbl-plag-transf}. This gives ability to detect similar \emph{structure} rather than similar \emph{tokens}, which are more vulnerable to simple transformations.

\paragraph{Tools}\mbox{}\\
Because plagiarism is kept as a serious offence in academia, a lot of various detection software has been made for it. Novak lists seven of the most well-known tools in a review \cite{RSCAD2016}: \emph{MOSS, JPlag, SPLaT, SIM, Marble, Plaggie} and \emph{Sherlock}. These tools can be classified into five different categories based on methods used: text, token, graph, tree and hybrid. Among these tools, the most common way to detect plagiarism is a five step approach: pre-process documents, tokenize documents, exclude templates, calculate similarities and find suspects using the similarity scores.

For example JPlag is tool targeted for Java code \cite{prechelt2002finding}. It works by utilizing the program structure, transforming the program code into sequence of tokens by traversing the parse tree of a program and using predefined token correspondence to form a token stream which represents the source code. To form similarity score between two programs, a string matching algorithm \emph{Greedy String Tiling} \cite{SSGST1993} is applied and the summed length of all matches is calculated. Resulting a similarity score of 1.0, if two programs are exact copies. Makers of JPlag also claim that it is resilient against most common obfuscations made by plagiarists. 

\subsection{Similarity detection} \label{chap-sd}

Similarity detection, or code clone detection, focuses directly on finding similar functionality between arbitrary pair of source codes $\{d_i, d_j\}$. We define it formally as following

\newtheorem*{smd1}{Similarity detection}

\begin{smd1}
Given a set of source code documents $D = \{d_1,...,d_n\}$ called corpus, define normalized similarity function $sim: d_i, d_j \rightarrow [0, 1]$ where $1 \leq i, j \leq n$, such that $sim(d_i, d_j) = sim(d_j, d_i)$ and $sim(d_i, d_i) = 1$, with a optional threshold $\theta \in [0, 1]$ that defines the limit where two source codes are considered as too similar. With this definition, any pair of source code file $(d_i, d_j) \in D \times D$ can also be presented as a triplet $(d_i, d_j, s)$, where $s$ is the similarity value between documents. 
\end{smd1}

\noindent
Above definition is flexible enough to support multiple solutions for the same task, the only restriction being that one must define a function that is able to return a score that either captures the textual or functional similarity. Defined method should be as resilient as possible against transformations given in table \ref{tbl-plag-strat} and table \ref{tbl-plag-transf}, so that two logically similar programs will get a high score even if their textual information differs. Scoring here refers to the value given by similarity function, which can be thought as a distance between documents.

The approach one takes, is ultimately based on how source code document is seen as a data \cite{Roy:2009:CEC:1530898.1531101}: document consisting of plain text, series of tokens, syntax tree, series of metrics or as a graph. The textual approach is based on detecting fragments of copy and paste, and requires no pre-processing if one uses naïve methods like string matching. Another way of approaching is by selecting a specific analysis method: attribute-counting-metrics, which are high-level metrics or by structure-metrics which are able to capture more low level representation of the source code \cite{Verco:1996:SDS:369585.369598}. Despite the approach, some kind of pre-processing and normalization is practically always used to prevent the method to be confused with common obfuscation strategies. 

The core process to detect similarity can be visualized as in figure \ref{fig-sd-flow}, which follows the general structure seen in many state of the art systems \cite{Roy:2009:CEC:1530898.1531101}.

\begin{figure}[!ht]
\centering
\vspace{0.5cm}
\scalebox{0.75}{
   \begin{tikzpicture}[node distance=2cm, baseline]

    \node (start) [io] {Corpus};
    \node (pro1) [process, right of=start, xshift=4.1cm] {Pre-process};
    \node (pro2) [process, right of=pro1, xshift=5cm] {Transform};
    \node (pro3) [process, below of=pro2] {Clone detection};
    \node (end) [io, below of=start] {Suspects};
    
    \draw [arrow] (start) --  node[anchor=south] {Raw source code} (pro1);
    \draw [arrow] (pro1) -- node[anchor=south] {Partition} (pro2);
    \draw [arrow] (pro2) -- node[anchor=east] {Intermediate representation} (pro3);
    \draw [arrow] (pro3) -- node[anchor=north] {Pairwise similarity} (end);
    
    \end{tikzpicture}
}
\caption{Similarity detection process. Corpus is transformed into a intermediate form so similarity scores can be calculated.}
\label{fig-sd-flow}
\end{figure}

\noindent
In figure \ref{fig-sd-flow} after the corpus has been set, pre-process stage takes as an input the unmodified source codes to perform two key tasks: to remove unnecessary segments and to determine the level of comparison granularity. The granularity one chooses can range from function-level to document-level, depending how accurately the results should pinpoint code reuse. After the source code has been partitioned, it is transformed into intermediate representation which consists of two parts: extraction that transforms the data to be used for clone detection algorithm \eg by parsing, tokenizing or building a control flow; and normalization. In latter one can apply techniques which reduce the noise between source codes: comments and whitespace removal, uniforming user-defined identifiers and/or removing syntactic noise.

The most key issue regarding to similarity detection, are false-positives that can be handled by manual verification after the suspects are gathered \cite{Verco:1996:SDS:369585.369598, Roy:2009:CEC:1530898.1531101}. This is often mandatory as detection algorithms can't completely understand nuances related to actual plagiarism \eg what is considered as being too similar. 


%\begin{minipage}{.45\hsize}
%\begin{lstlisting}[language=Java, caption=Example code 1]{asd1}
%void code()
%{
%
%}
%\end{lstlisting}
%\end{minipage}\hfill
%\begin{minipage}{.45\hsize}
%
%\begin{lstlisting}[language=Java, caption=Example code 2]{asd2}
%void code()
%{
%
%}
%\end{lstlisting}
%\end{minipage}


\subsection{Authorship identification} \label{chap-ai}

Authorship identification deals with the issue of trying to name the author of a document given some previous work of the author. This problem can be seen as a classification task \cite{KRSUL1997233}, thus we can define authorship identification formally as following.

\newtheorem*{aui1}{Authorship identification}
\begin{aui1}
Given a set of documents $D$, a set of authors $A$ and a function $f: D \rightarrow A$ that identifies the writer by assigning every source code document $d \in D$ to one author $a \in A$. Estimate $f$ with $\hat{f}$, a classifier that treats every document as a feature vector $\bolditt{x}$ and every known class as a vector $\bolditt{y}$. The predicted author $\hat{y}$ can be thus expressed with $\hat{f}(\bolditt{x}) = \hat{y}$.
\end{aui1}

\noindent
This classifier should be able to discriminate between writing styles of different authors, which in programming refers to the habits \ie programming style. Habits are restricted by the grammar of the chosen programming language, but commonly refers to everything that is controllable by the author \eg how one names variables or uses spacing. As a problem, it's therefore very opposite to the similarity detection where we want to find maximum equivalency between programs. 

One way to represent programming style is by using software metrics \cite{KRSUL1997233}. Software metrics can be put into roughly three categories: layout being fragile metrics which are easily transformed by the IDE, style which are non-fragile metrics related to layout and lastly structure which can capture experience and ability of the programmer. As source code can be also thought as a text written with a specific language, majority of the methods using a natural language can be applied, thus we can form a five-level categorization for stylistic features called \emph{stylometrics features}  \cite{Stamatatos:2009:SMA:1527090.1527102}. Categorization for stylometric features is visualized in table \ref{tbl-ai-stylomet}, where semantic features are the most difficult to form as they require understanding deeper meaning of the written source code.

\begin{table}[ht]
\centering
\caption{Five-levels of stylometrics features. }
\label{tbl-ai-stylomet}
\begin{tabular}{|c|c|} \hline
\textbf{Category}             & \textbf{Feature examples} \\ \hline
Lexical              & Token statistics, word n-grams                   \\
Character            & Character n-grams, types, compression            \\
Syntactic            & Errors, expressions, keywords, parse tree        \\
Semantic             & Synonyms, functional dependencies                \\
Application-specific & Indentation, language-specific \\ \hline
\end{tabular}
\end{table}

In natural language authorship analysis, statistical methods are often being used \cite{Stamatatos:2009:SMA:1527090.1527102}, and more specifically machine learning to find reoccurring patterns that are able to distinguish between writing styles. The training of these statistical models can happen in two ways: via profile-based or via instance-based. In profile-based approach, all documents that are presented as training data for an author  are concatenated into one file. In instance-based learning, each text is used as an individual data point. 

Whichever the choice is, one is able to reduce the problem into multiclass classification \cite{Stamatatos:2009:SMA:1527090.1527102}, where author pool of size $n$ can be encoded into binary vector $\bolditt{y} = [y_1, y_2, \cdots, y_n], y_i \in \{0, 1\}$. With this representation if the $i$th candidate is the actual author, the value of that field will be one and the rest are zeroes. 


\subsection{Information retrieval} \label{chap-IR}

Information retrieval (IR) is a collection of strategies of finding documents from large collections \cite{Manning:2008:IIR:1394399}. These documents are often represented as unstructured text and possible methods covers topic like: clustering documents to find similar groups, classifying documents based on their content, ranking text for query search and building search engines. In this thesis as we mainly focus on finding similarities between documents and how to classify the author, we disregard some of the query-based focus of IR and use techniques that are relevant to plagiarism study \ie how document can be represented for machine learning models and how distance between two documents can be calculated.




\subsubsection{Document representation} \label{chap-IR-document-repr}

As we need some form of numerical way \ie vector to represent one document, we use following IR-related concepts to express the documents in our corpus: \emph{vector space model} which captures algebraically the representation of the document and a \emph{weighting scheme} which helps to give more importance to specific terms.


\paragraph{Vector space model}
%Tokenization

One form of vector space model is called \emph{a binary term-document incidence matrix} \cite{Manning:2008:IIR:1394399}, which represents documents as columns of the matrix and terms as the rows. Terms are gathered by tokenization procedure which divides single document into units that are often \emph{words} of the document, but can also be \eg adjacent words. Let $\bolditt{M}_{n \times k}$ be this matrix having $n$ terms and $k$ documents, then the value of $\bolditt{M}_{d, t}$ \ie term $t$ appearing in document $d$, is 1 if it appears at all and zero otherwise. Table \ref{tbl-binmatr} shows example matrix build from programs in appendix \ref{appendix:programs}.  

\begin{table}[ht]
\centering
\caption{Example of a binary term-document incidence matrix for three sample programs in appendix \ref{appendix:programs}.}
\label{tbl-binmatr}
\begin{tabular}{|c|c|c|c|} \hline
      \backslashbox{\bf Term}{\bf Document} & A & B & C \\ \hline
\texttt{public} & 1 & 1 & 1 \\
\texttt{sum}   & 0 & 1 & 0 \\
\texttt{double} & 1 & 1 & 0 \\
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
\texttt{b} & 1 & 1 & 1 \\ \hline
\end{tabular}
\end{table}

\noindent
Taking a transpose of the matrix in table \ref{tbl-binmatr} gives a document-term matrix, where one row represents now document having $n$ dimensions reserving one dimension per each term occurrence. For example $\bolditt{a} = [1, 0, 1, \cdots, 1]$. This is referred also as the \emph{bag of words} model, because it treats every word as independent event, losing the information about ordering \cite{Manning:2008:IIR:1394399}. Unigrams can be thus expressed as a product of term probabilities 

\begin{equation}
    P(t_1, t_2, t_3, t_4) = P(t_1)P(t_2)P(t_3)P(t_4)
\end{equation}

A binary term-document incidence matrix is however very naïve, giving the same value despite the times term appears in a document. The solution is to use a  method called \emph{term weighting}, which is able to assign non-binary value for terms. 

\paragraph{Term weighting schemes}

One simple scheme is called \emph{term frequency}, which is the occurrence of term $t$ in document $d$ denoted by $tf_{t, d}$ \cite{Manning:2008:IIR:1394399}. Let $f_{t, d}$ denote the raw frequency count, then term frequency can be given as $tf_{t, d} = f_{t, d}$ and normalized by dividing the raw frequency with the total frequency over every term in document

\begin{equation} \label{eq-tf}
    tf_{t, d} = \dfrac{f_{t, d}}{\sum \limits_{t' \in d} f_{t', d}}
\end{equation}

To scale down the most frequently appearing terms, one can use \emph{inverse document frequency} which boosts the weights of rare occurring terms \cite{Manning:2008:IIR:1394399}. Inverse document frequency is defined as

\begin{equation}
    idf_t = \log \dfrac{N}{df_t}
\end{equation}

\noindent
Where $N$ is the total amount of documents and $df_{t}$ is the count of documents that contains the term $t$.

Now by taking the product of term frequency and inverse document frequency (tf-idf), we get a weight for each term appearing in a document. It's defined as

\begin{equation}
    tf{\text -}idf_{t, d} = tf_{t, d} \cdot idf_{t}
\end{equation}

By using tf-idf weighting scheme, we are able to discriminate between documents and diminish the problem of frequently appearing terms that are introduced often in programming, because the language is very structured and defined by a finite amount of keywords and variables. The representation of documents remains similar to Table \ref{tbl-binmatr}, having term weights to ultimately describe the document.

\subsubsection{Document similarity} \label{chap-bg-sim}

Now as we have a way of expressing a document as a vector, we need to somehow be able to calculate similarity between two documents, which is crucial part for similarity detection. One way of doing this is by using \emph{cosine similarity}.

\paragraph{Cosine similarity}

Cosine similarity measures the similarity between two documents by calculating the cosine of the angle between the vector representations \cite{Manning:2008:IIR:1394399}. Let $\bolditt{x}, \bolditt{y}$ be these vectors for documents $d_1, d_2$, then

\begin{equation} \label{eq-cosine-orig}
    sim(d_1, d_2) = \cos(\theta) = \dfrac{\bolditt{x} \boldsymbol{\cdot} \bolditt{y}}
                          {\norm{\bolditt{x}}_2 \norm{\bolditt{y}}_2} = 
                          \dfrac{\sum \limits_{i=1}^n x_i y_i}
                                {\sqrt{\sum \limits_{i=1}^n {x}_i^2} \sqrt{\sum \limits_{i=1}^n y_i^2}}
\end{equation}

\noindent
As the dot-product is normalized in equation \ref{eq-cosine-orig} with Euclidean norm and by using non-negative weights derived from tf-idf, the cosine similarity gets values between zero to one and values closer to one indicate from high content similarity. The complement of cosine similarity called \emph{cosine distance} can be calculated by $d = 1 - \cos(\theta)$ \cite{}.

\subsubsection{Retrieval metrics}

Having a way to retrieve candidate documents for possible plagiarism, creates a need to justify how well the model is performing. For evaluating the retrieval method in a binary case, three important metrics have been defined \cite{Manning:2008:IIR:1394399}: precision, recall and $F_1$-score. To express these metrics we use a confusion matrix which has four fields: true positive (TP), true negative (TN), false negative (FN) and false positive (FP). The meaning of these fields is visible from table \ref{tbl-confmatr-orig}.


\begin{table}[ht]
\centering
\caption{Confusion matrix \cite{Manning:2008:IIR:1394399}.}
\label{tbl-confmatr-orig}
\begin{tabular}{c|c|c}
          & \bf Relevant & \bf Irrelevant \\ \hline
\bf Retrieved & TP      & FP        \\
\bf Rejected  & FN      & TN       
\end{tabular}
\end{table}

Precision and recall can be now defined by using the confusion matrix. Both precision and recall calculate the rate of true positives to falsely retrieved documents, and balancing between these values requires often knowledge about the domain. If precision is preferred to be high, model is able to correctly retrieve greater portion of correct positive cases and if recall is preferred, model is able to retrieve high portion of relevant documents. 

\begin{align}
    \text{Precision } &= \dfrac{TP}{TP + FP} = \dfrac{|\text{relevant} \cap \text{retrieved}|}{|\text{retrieved}|}\\
    \text{Recall } &= \dfrac{TP}{TP + FN} = \dfrac{|\text{relevant} \cap \text{retrieved}|}{|\text{relevant}|}
\end{align}

$F_1$-score is slightly different metric, giving an average between precision and recall. It's defined as

\begin{equation}
    F_1 = 2 \cdot \dfrac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}

By using these metrics, we are able to evaluate the results of our detection process when we retrieve a set of documents that our model suspects from plagiarism. But to be able to properly evaluate, the data needs to be labeled \ie there must exist some gold standard about the confirmed cases of plagiarism, or the results needs to be confirmed by a human with domain knowledge \eg course administrative staff. 

\subsection{Document classification} \label{chap-bg-classification}

The problem of classification can be formulated as $\gamma: \mathbb{X} \rightarrow \mathbb{C}$ \cite{Manning:2008:IIR:1394399}, approximating a function $\gamma$ that maps data $\bolditt{x} \in \mathbb{X}$ to class $c \in \mathbb{C}$. For example a \emph{binary classifier} would choose between a positive and a negative class $\mathbb{C} = \{+, -\}$, and \emph{multiclass classifier} between multiple classes $\mathbb{C} = \{c_1, c_2, \cdots, c_n\}$.

To be able to classify documents algorithmically into some predefined classes, the classifier must learn a way to predict outputs from inputs \cite{hastie_09_elements-of.statistical-learning}. That is, given some $d$-dimensional data $\bolditt{x} \in \mathbb{R}^d$, it must predict the response variable $y$ which can be encoded into binary vector. To make the prediction, the classifier must be supported with some observed data as a training data represented in matrix format $\bolditt{X}_{n \times d}$ and predefined response variables in a column vector $\bolditt{y}_{n \times 1}$. 

As we know the output $y$ of each observed point $\bolditt{x}$, we want to have similar data to be predicted with the same output. The prediction the algorithm gives can be noted as $\hat{y}$ for $\bolditt{x}$, and because this value is just a prediction, evaluation is needed for the algorithm to be able to chance its learning into right direction. This evaluation happens by penalizing wrong predictions with a loss function $L: \hat{\theta} \times \theta \rightarrow \mathbb{R}$ \cite{hastie_09_elements-of.statistical-learning}. For example one loss function able to penalize categorical predictions, is called \emph{0-1 loss} and formulated as $L(\hat{y}, y) = I(\hat{y} \neq y)$. This kind of setting is also called as \emph{supervised learning} as we have some data that can guide the algorithm into right direction, the analogue for supervised learning can be thought as learning with a teacher \cite{hastie_09_elements-of.statistical-learning}.


%IR pgs. 259 - 273, Element. stat pgs. 37, 21

\paragraph{Naïve Bayes}

%A Comparison of Event Models for Naive Bayes Text Classification

Naïve Bayes is a probabilistic model, which is often used as a baseline model in text classification \cite{acemNBtc2001}. It applies the \emph{Bayes theorem} to estimate the parameters of the classifier \ie conditional probabilities with respect to data. Bayes theorem is generally given for events $A, B$ as 

\begin{equation} \label{eq-naive-bayes}
    P(A \mid B) = \dfrac{P(A \cap B)}{P(B)} = \dfrac{P(A)P(B \mid A)}{P(B)}
\end{equation}

\noindent
Where $P(B)$ can be expressed with the law of total probability
\begin{equation}
    P(B) = P(B \mid A)P(A) + P(B \mid \lnot A)P(\lnot A)
\end{equation}

When the area of interest is classification, we denote the probabilities of events $A,B$ as the \emph{prior} and \emph{likelihood}. Prior being in this case $P(y=c), \; c \in \mathbb{C}$; the probability of a class appearing in data and likelihood $P(\bolditt{x} \mid y=c)$; the likeliness of a document belonging to a class. Equation \ref{eq-naive-bayes} can be therefore rewritten for classification as \cite{Zhang04theoptimality, acemNBtc2001}

\begin{equation} \label{eq-naive-bayes-class}
    P(y \mid \bolditt{x}) = \dfrac{P(y)P(\bolditt{x} \mid y)}{P(\bolditt{x})} = \dfrac{P(y)P(\bolditt{x} \mid y)}{\sum \limits_{c \in \mathbb{C}} P(y=c)P(\bolditt{x} \mid y=c)}
\end{equation}

In Equation \ref{eq-naive-bayes-class}, the denominator remains constant because $\bolditt{x}$ is kept unchanged, and therefore it is proportional to product between prior and likelihood

\begin{equation}
    P(y \mid \bolditt{x}) \propto P(y)P(\bolditt{x} \mid y)
\end{equation}

Because the underlying real distribution is unknown, we need to estimate the prior and likelihood from the training data \ie data we are keeping as observations. Estimated prior can be calculated from the relative frequency $\hat{P}(c) = \frac{\#c}{|\mathbb{X}|}$, number of samples belonging to the class $c$ divided by the total number of observations \cite{Manning:2008:IIR:1394399}. To calculate the estimated likelihood $\hat{P}(\bolditt{x} \mid y)$, one uses the assumption that features represented in $\bolditt{x}$ are conditionally independent with respect to each other. Therefore likelihood can be simplified by using the chain rule \cite{Manning:2008:IIR:1394399}

\begin{equation}
    \hat{P}(\bolditt{x} \mid y) = \hat{P}(x_1, x_2, \cdots, x_n \mid y) = \prod \limits_i^n \hat{P}(x_i \mid y)
\end{equation}

To assign data into a class, the most probable class is chosen \cite{Zhang04theoptimality, acemNBtc2001, Manning:2008:IIR:1394399}. This is referred to also as \emph{maximum a posteriori} (MAP), and the final class assignment \ie the result of classifier $\gamma$ is expressible as

\begin{equation} \label{eq-mapp}
    \hat{y} = c_{\text{map}} = \argmax \limits_{c \in \mathbb{C}} \hat{P}(c) \prod \limits_i^n \hat{P}(x_i \mid c)
\end{equation}

\noindent
That is, the most likely class for a data point is the class which maximizes the posterior, which is proportional to calculating product between prior and posterior.  

A variant of Naive Bayes called \emph{Multinomial Naïve Bayes}, is able to form the likelihood by assuming underlying multinomial distribution \cite{acemNBtc2001}. Given the problem of document classification and that the feature vector $\bolditt{x}$ represents term frequencies of vocabulary $\mathbb{V}$ built from the training data, the conditional probability of Equation \ref{eq-mapp} can be given in similar way as term-frequency function in Equation \ref{eq-tf}. Contrast to it we also add a smoothing called \emph{Laplace smoothing}, to eliminate the problem with terms appearing zero times \cite{Manning:2008:IIR:1394399}

\begin{equation} \label{eq-laplace}
    \hat{P}(x \mid c) = \hat{P}(t \mid c) = \dfrac{f_{t, c} + 1}{\sum \limits_{t' \in \mathbb{V}} (f_{t', c} + 1)} 
\end{equation}

\noindent
In Equation \ref{eq-laplace} $f_{t, c}$ is the frequency of term $t \in \mathbb{V}$ appearing in class $c \in \mathbb{C}$, so the conditional probability of a data point given class can be calculated with smoothed relative frequency of the term that the point represents. However, it has been shown that using tf-idf weigthing scheme can improve the classification results even though the values are non-discrete \cite{Kibriya:2004:MNB:2146834.2146882}.


\subsection{Document clustering}

Document clustering is a process that is able to group the set of documents, so that their similarities is maximized \ie documents belonging to the same group are as similar as possible. This is relatively easy task for a human to do manually for a small set of documents, but in order to perform this task automatically in large scale we resort to \emph{unsupervised learning}. 

Unsupervised learning can divide the observed data \ie documents, into subgroups called \emph{clusters} \cite{hastie_09_elements-of.statistical-learning}. The main difference to supervised learning (classification) is that when the data is represented as a sequence $X = (\bolditt{x}_1, \bolditt{x}_2, \cdots, \bolditt{x}_n)$, where $\bolditt{x}_i \in \mathbb{R}^d$ is $d$-dimensional feature vector which represents the $i$th document, we don't have the sequence of response variables $\bolditt{y} = (y_1, y_2, \cdots, y_n)$ to guide the process. Thus there is no loss function which is dependent from the true label of the data, requiring the distribution of the data to determine the class \cite{Manning:2008:IIR:1394399}. The performance of the unsupervised model can be very subjective, requiring some kind of prior domain knowledge as there might not exist any ground truth for the data \cite{hastie_09_elements-of.statistical-learning}. 

The Figure \ref{fig-clust-example} visualizes data generated from three separate distributions. If we know how the data was generated, it would be easy by prior knowledge and by visually, to divide the space into three regions. However, if the data would be more uniformly distributed and one could not say the exact amount of regions, then this task requires more knowledge about how two data points are able to have similar location. When considering for example plagiarism between documents, we are highly interest of cases where just two or more documents are too similar to each others. In Figure \ref{fig-clust-example} these would mean for example all data points that are touching each others; a more fine grained classification with a lot more clusters than just three. 

\begin{figure}[!h]
\centering
\setlength\figureheight{7cm}
\setlength\figurewidth{7cm}
\input{plots/example_datadistr_clust.tikz}
\caption{Data drawn from three different distributions. Three major clusters are clearly visible as they are formed from three different distributions.} \label{fig-clust-example}
\end{figure}

\newpage

The normalized similarity value $s \in [0, 1]$, or respectively distance value $d = 1 - s$, is defined before the clustering algorithm is executed, and it ultimately controls what kind of cluster are being formed. Distances between data points can be precomputed into matrix of documents $\bolditt{M}_{d \times d}$, where $\bolditt{M}_{i, j}$ is the similarity, or distance value between two documents. 

We define the problem of clustering first formally and then show two different unsupervised clustering algorithms: \emph{K-means clustering} and \emph{DBSCAN}.

\newtheorem*{docclus}{Document clustering}
\begin{docclus}
Given a set of datapoints $X = \{\bolditt{x}_1, \bolditt{x}_2, \cdots, \bolditt{x}_n\},  \; \bolditt{x}_i \in \mathbb{R}^d$, representing the documents, define assignment $\gamma: X \rightarrow \{1, \cdots, k\}$ where $k$ is the total amount of clusters \cite{Manning:2008:IIR:1394399}. The set of clusters can be notated by $\Omega = \{\omega_1, \omega_2, \cdots, \omega_k\}$ and each document belongs to some cluster $\forall d \in \omega$. 
\end{docclus}


\paragraph{K-means clustering}

K-means clustering requires the $k$ parameter to be predefined and it assumes there exists a \emph{centroid} \ie a mean point, for every cluster $C = (\boldsymbol{\mu}_1, \boldsymbol{\mu}_2, \cdots, \boldsymbol{\mu}_k), \; \boldsymbol{\mu}_i \in \mathbb{R}^d$ \cite{Manning:2008:IIR:1394399}. To assign a point to a cluster, one calculates the \emph{squared Euclidean distance} to a centroid $\norm{\bolditt{x}_i - \boldsymbol{\mu}}^2$, and minimizes this distance \ie assigns data point to the same cluster as the nearest centroid. The algorithm works iteratively updating the cluster assignments for each point and calculating new centroids until the algorithm converges. The following pseudocode shows the complete algorithm

\begin{algorithm}[ht]
\caption{K-means algorithm \cite{Manning:2008:IIR:1394399}}
\label{alg-kmeans}
\begin{algorithmic}

\Require Set of datapoints $X$
\Require Amount of clusters $k$
\Procedure{K-means}{$X, k$}
   \State $C  \leftarrow $ \Call{InitCentroids}{$X,k$}
   \While{stop criterion has not been met}
       \For{$i=1$ to $k$}
            \State $\omega_i \gets \{\} $
       \EndFor
        \For{$j=1$ to $|X|$}
            \State $l \gets \argmin_{l} \norm{\bolditt{x}_j - \boldsymbol{\mu}_l}^2$
            \State $\omega_l \gets \omega_l \cup \bolditt{x}_j$
        \EndFor
       \State $C \gets $ \Call{UpdateCentroids}{$\Omega$}
   \EndWhile
\State \textbf{return} $C$
\EndProcedure

\end{algorithmic}
\end{algorithm}

\noindent 

In Algorithm \ref{alg-kmeans} centroids are first initialized randomly from the set of data points. Then iteratively until stop criterion is met, which is that there were no updates to centroids $C$, $k$ clusters are first initialized and then from the set of data points cluster assignments are calculated. Data point is assigned to nearest cluster with respect to Euclidean distance. After every loop, new centroids are calculated by taking the mean of assigned data points per cluster. 

Visualization of the clustering result using K-means for the same data as in Figure \ref{fig-clust-example} is seen below.

\begin{figure}[ht]
\centering
\setlength\figureheight{7cm}
\setlength\figurewidth{7cm}
\input{plots/example_kmeans.tikz}
\caption{Result of K-means clustering after converge. Crosses indicate the cluster centroids, other colors cluster assignments. Parameter $k$ is set to 3 so three different clusters have been discovered by the algorithm.} \label{fig-kmeans-example}
\end{figure}


%https://www.youtube.com/watch?v=0MQEt10e4NM

The drawback with K-means clustering is that one must specify the parameter $k$ before the clustering, and when detecting plagiarism, there is no indication beforehand that how many documents should be grouped together. Therefore pre-estimating number of clusters can be hard. One solution is to utilize the density of the points rather than direct distance, which is what DBSCAN is cabable of doing.

\paragraph{DBSCAN}

Density-based spatial clustering of applications with noise (DBSCAN), can produce clustering by using only the density information, label some data points as noise, produce arbitrary sized clusters and use any distance function \cite{Ester:1996:DAD:3001460.3001507}. It requires two parameters $\varepsilon$ which controls the neighbour search radius, and $MinPts$ which defines the minimum number of points needed to form a cluster. To form a cluster a point $q$ must be reachable from point $p$ \ie there must be a path $p \leadsto q$ which fulfills the two parameter contraints. To form this path, some points are labeled as core points satisfying parameters simultaneously, and some as border points which have at least one core point in its $\varepsilon$-range. If a data points is neither above, it is labeled as noise \ie unassigned point.


The pseudocode for DBSCAN is given in Algorithm \ref{alg-dbscan}, where \textproc{DiscoverNeighbours} is a recursive function that finds the neighbourhood space by forming the radius based on the distance function, and retrieves all reachable points. The algorithm forms the amount of clusters itself if there are just data points to label as core points.

\begin{algorithm}[ht]
\caption{DBSCAN algorithm \cite{Ester:1996:DAD:3001460.3001507, Schubert:2017:DRR:3129336.3068335}}
\label{alg-dbscan}
\begin{algorithmic}

\Require Set of datapoints $X$
\Require Distance radius $\varepsilon$
\Require Minimum neighbour count $MinPts$
\Require Distance function $dist: X \times X \rightarrow \mathbb{R}$
\Procedure{DBSCAN}{$X, \varepsilon, MinPts, dist$}
   \State $k \gets 0$
   \For{$i = 1$ to $|X|$}
    \State $N \gets $ \Call{DiscoverNeighbours}{$X, dist, \bolditt{x}_i ,\varepsilon, MinPts$}
    \If{$\bolditt{x}_i$ is a core point}
        \State $k \gets k + 1$
        \State $\omega_k \gets N \cup \bolditt{x}_i$
    \Else
        \State $\bolditt{x}_i$ is noise
    \EndIf
   \EndFor
   \State \textbf{return} $\{\omega_1, \omega_2, \cdots, \omega_k\}$
\EndProcedure

\end{algorithmic}
\end{algorithm}


Using parameters $\varepsilon = 0.5, MinPts = 15$ and setting distance function as Euclidean distance, DBSCAN learns more denser clusters than K-means and is able to label some data as noise, which is visible in Figure \ref{fig-dbscan-example} as black data points. If these data points would be document representation in Euclidean space, we can interpret noise points as documents which are free from plagiarism \ie they are unique programs which are not similar to other authors work. These three clusters then shows the possible plagiarists using three different approaches and might result to three different real life groups who have worked collaboratively. 

\begin{figure}[ht]
\centering
\setlength\figureheight{7cm}
\setlength\figurewidth{7cm}
\input{plots/example_dbscan.tikz}
\caption{Result of DBSCAN by setting parameters $\varepsilon = 0.5, MinPts = 15$. Some data has been labeled as noise, as they are too far away from core points.} \label{fig-dbscan-example}
\end{figure}

