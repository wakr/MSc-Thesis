In this chapter we turn the focus to the actual methods used in these studies. We use the same categorization as a baseline for studies that was made earlier. The formal notation used in studies is generalized to match the style of this paper, meaning that a single element of a set and scalars are represented as lower-cased italics, matrices are bold and upper-cased, vectors are bold but lower-cased, tree structures as capitalized $T$ and segments of source codes as capitalized $S$, often implying strings. We use words \emph{term, token} and \emph{word} as a synonym for a single sequence of characters which are usually divided by spaces.

\subsubsection{Similarity detection}
The problem of similarity detection is described formally in chapter \ref{chap-sd} and we use that as a general high-level baseline. We will focus in similarity detection studies mainly to the actual similarity measure, as it's described often in following 17 studies.


The attribute counting study by Moussiades and Vakali uses a graph clustering on top of pair-wise similarities calculated using the Jaccard coefficient \cite{PACASCD2005}. Authors use following form of Jaccard coeficcient in their study where $A$ is the indexed set of substitute keywords per source code 

\begin{equation}\label{jacc_eqn}
    sim(d_1, d_2) = \dfrac{|A(d_1) \cap A(d_2)|}{|A(d_1) \cup A(d_2)|}
\end{equation}
\noindent
% refer to plag. attack
The indexed set can be built considering language dependent keywords \eg \texttt{while, for, false and true} in \cpp, and marking their position with respect to the occurrences of same keywords previously. However, authors claim that to generalize the set more, substitution keywords should be used. This means that for example all occurrences of \texttt{for}- and \texttt{while} -loops should be counted together, which helps to protect against plagiarism attack. The graph clustering algorithm Moussiades and Vakali uses is called \emph{WMajorClust} which works by presenting all pairs of source codes as non-directed graph $G = (V, E)$ where the set of vertices $V$ represents the source codes while the set of edges $E$ are weighted by equation \ref{jacc_eqn}. We can also express the definition of $E$ by Moussiades and Vakali with following constraints

\begin{equation}\label{jacc_edges_eqn}
         E = \Big\{ \{ d_i, d_j, sim(d_i, d_j)\} \, | \, (d_i, d_j) \in D \times D \land sim(d_i, d_j) \geq \theta \Big\}
\end{equation}

\noindent
%chapter ref
In equation \ref{jacc_edges_eqn}, $\theta$ is a user-defined parameter and works as a minimum threshold value that separates non-plagiarized source codes from plagiarized ones \ie two source codes will not share an edge if their similarity is below $\theta$.

Segment matching study by Brixtel \etal presents their algorithm, which builds from three major steps \cite{LICD2010}: pre-filtering, segmentation and document distance calculation. Their pre-filtering is to normalize the source code in a way, that every keyword and parameter definitions is transformed into a single symbol. As a segmentation, authors split the source code by lines forming set of segments $S_k$ presenting the partitioned set of a single source code. Similarity calculation happens by first forming distance matrix $\bolditt{M}$ between two source codes $d_1, d_2$ and then comparing all pairs of segments $(s_i^1, s_j^2) \in S_1 \times S_2$ where $S_k = (s_1^k, ..., s_n^k)$, with \emph{Levenshtein edit distance} \cite{BCCCDIR1966}. Distance matrix $\bolditt{M}$ is then transformed into noise reduction matrix $\bolditt{H}$ by finding the maximal matching between segmentations. Finally, $\bolditt{H}$ is filtered into a matrix $\bolditt{P}$ by convolution and utilizing a threshold\footnote{Authors used $\theta = 0.7$}. With the matrix $\bolditt{P}$, distance between two pairs of documents is calculated by Brixtel \etal as 

\begin{equation}
    sim(d_1, d_2) = 1 - \dfrac{1}{\min(|S_1|, |S_2|)}\sum_{i, j} 1 - \bolditt{P}_{(i, j)}
\end{equation}

\noindent
Zhang and Liu utilize AST and their core method is mainly constructed from two methods \cite{ASTMLPD2013}: forming the AST-representation and similarity calculation. Their AST-representation is done by traversing the tree structure and turning it into textual format by printing the nodes, and similarity calculation is computed using \emph{Smith Waterman algorithm} that finds the optimal matching between two strings $S_1, S_2$ \cite{SMITH1981195}. Zhang et Liu gives the form for similarity calculation between two source codes as

\begin{equation}
    sim(d_1, d_2) = \dfrac{2 \cdot \text{ SLength}(d_1, d_2)}{|S_1| + |S_2|}
\end{equation}
\noindent
Where SLength is the length of maximal matching string obtained via  \emph{Smith Waterman algorithm}, and $|S_k|$ represents the character length of one segment. 


$N$-gram studies take a different approach. Cosma and Joy uses \emph{Latent Semantic Analysis} to find suspicious documents \cite{AASCPD2012}. They first preprocess the documents by removing \eg short terms and comments. Then all documents are transformed into a term-by-file matrix $\bolditt{A}$, which presents each source code as occurrences of each possible unique term, being same as forming the unigrams of a document. Values of $\bolditt{A}$ are weighted, and then $\bolditt{A}$ is decomposed via \emph{singular value decomposition} into $\bolditt{A} \approx \bolditt{A}_k  = \bolditt{U}_k\mathbf{\Sigma}_k\bolditt{V}_k^\intercal$ where $\bolditt{U}$ represents terms by dimension, $\mathbf{\Sigma}$ singular values and $\bolditt{V}$ files by dimensions. The dimensionality reduction is performed for all these matrices by considering only the first 30 columns represented by the subscript $k$. Finally, the similarity between a query vector $\bolditt{q}$ representing term frequency of document $d_i$, and document $d_j$ represented as a column $\bolditt{a}_j$ of matrix $\bolditt{A}$ is calculated by using \emph{cosine similarity} \cite{AASCPD2012}

\begin{equation}\label{cosine_sim_eqn}
    sim(\bolditt{q}, d_j) = \cos \Theta_j = \dfrac{\bolditt{a}_j^\intercal \bolditt{q}}{\norm{\bolditt{a}_j}_2 \norm{\bolditt{q}}_2} = \dfrac{\bolditt{a}_j \boldsymbol{\cdot} \bolditt{q}}{\sqrt{\sum \limits_{i} \bolditt{a}_{(j, i)}^2} \sqrt{\sum \limits_{i} \bolditt{q}_i^2}}
\end{equation}

\noindent
Acampora and Cosma \cite{AFAPLI2015} continues on same style as Cosma and Joy \cite{AASCPD2012}, first preprocessing the documents by lowercasing and removing comments, syntactical tokens and short terms. Then using singular value decomposition with weighting to form three matrices from the corpus of source codes. For the reduced matrix $\bolditt{V}$ however, they perform a \emph{Fuzzy C-Means} clustering algorithm, which is tuned with \emph{ANFIS} learning algorithm to optimize the hyperparameters of Fuzzy C-means \cite{AFAPLI2015}. The process returns a membership degree $\mu_{i, k}$ per document, indicating how close $i$th document is to $k$th cluster. 
\noindent
Flores \etal \cite{USCR2014} uses similar preprocessing approach to Cosma and Joy. They first process the documents by lower-casing them and removing repeated character, tabs with spaces. Then transform the documents into $3$-grams and weighting them by using a \emph{term frequency}. Finally, similarity is calculated using cosine similarity where $t$ is one of the 3-grams and $tf$ is the term frequency function \cite{USCR2014}. Formally this can be calculated in a same way as in equation \ref{cosine_sim_eqn} between two documents as

\begin{equation}
    sim(d_i, d_j) = \dfrac{\sum\limits_{t \in d_i \cap d_j} tf(t, d_i) tf(t, d_j) }
                          {\sqrt{\sum\limits_{t \in d_i} tf(t, d_i)^2 \sum\limits_{t \in d_j} tf(t, d_j)^2}}
\end{equation}

\noindent
Heblikar \etal \cite{Heblikar2015NormalizationBS} preprocesses also their documents by lower-casing, pruning repeated whitespace and removing single symbols. They then normalize the documents by considering most frequent terms, renaming similar terms under same symbols and ultimately filtering them completely out from the source codes. For detection phase, they use same approach as Flores \etal did in \cite{USCR2014} but use both 1-grams and 2-grams with \emph{term frequency - inverse document frequency} (tf-idf) weighting. Interestingly, also Ramírez-de-la-Cruz \etal in \cite{OTIOLSS2015} and Ramírez-de-la-Cruz \etal in \cite{ramirez2015high} decides to use cosine similarity and Jaccard coefficient. The only major difference being, that Ramírez-de-la-Cruz \etal uses additional structural and stylistic features, forming total combination of eight various similarity measurements \cite{OTIOLSS2015}. Where as Ramírez-de-la-Cruz \etal in \cite{ramirez2015high} uses cosine similarity with character 3-grams to calculate five different similarities: lexical, stylistic, comments, text\footnote{Referring here as any string passed in as an argument of a function} and structural. Lastly, Ohmann and Rahal proposes density-based clustering to form clusters of similar documents \cite{Ohmann2015}. Their similarity approach follows closely to other studies presented above: filtering and normalization as preprocessing, data format as word $n$-grams and similarity values gained by using cosine similarity. The only major difference contrast to other studies is, that they perform tokenization that transforms source code into predefined set of tokens \eg integer declarations are changed to string "DN".

Tree-based studies mostly relies on calculating similarity between two tree structures $T_i, T_j$ obtained from the original documents $d_i, d_j$ by parsing them. For example Ng \etal first generate a parse tree $T$ from the source code, then decompose the parse tree into subtrees $T' \subseteq T$ with respect to the functionality \eg imports are categorized together \cite{TBCFPD2012}. The similarity score is calculated by traversing trees with \emph{depth-first search} and summing the node and token similarities for all subtrees. As a function of similarity between two documents, this is defined below where $simST$ is a subtree similarity

\begin{equation}
    sim(d_i, d_j) = sim(T_i, T_j) = \dfrac{\sum\limits_{i, j}simST(T'_i, T'_j)}{10 \cdot |T'|} \cdot 100
\end{equation}

\noindent
Son \etal computes similarity value between two parse trees with a modified parse tree kernel, and argue that their kernel function is able to consider also the length of the document \cite{AAPSCDPTK2013}. They define the kernel function $k$ via recursive function $C$ where $n$ is a node of a subtree $T'$. Function $C$ finds a maximal similarity between two nodes thus authors calls it also as the \emph{maximum node value}  

\begin{equation}
    k(T_i, T_j) = \sum\limits_{n_i \in T'_i} \sum\limits_{n_j \in T'_j} C(n_i, n_j)
\end{equation}

\noindent
This kernel function captures the similarity between two tree structures and the normalized similarity function is defined as \cite{AAPSCDPTK2013}

\begin{equation}\label{norm_kern_eqn}
    sim(d_i, d_j) = \dfrac{k(T_i, T_j)}{\sqrt{k(T_i, T_i) \cdot k(T_j, T_j)}}
\end{equation}

% C(n_i, n_j) &= \lambda \prod \limits_{k}^{nc(n_i)} \left( 1 + \max\limits_{ch \in ch_{n_j}} C(ch_k(n_i), ch)\right)

\noindent
Another study that utilizes kernel between tree structures is by Fu \etal \cite{Fu2017WASTKAW}. They first build abstract syntax tree from a source code by normalizing and weighting nodes with tf-idf, then use a tree kernel to measure similarity between two tree structures. This tree kernel is defined as

\begin{equation}
    k(T_i, T_j) =  \sum\limits_{t_i \in T_i} \sum\limits_{t_j \in T_j} \left(\lambda \cdot \text{ dist}(\text{word}_{t_i}, \text{word}_{t_j}) \cdot w_{t_i, T_i} \cdot w_{t_j, T_j}\right)
\end{equation}

\noindent
Where $\lambda$ is a decay factor penalizing tree height, dist is the edit distance between two string values, word$_t$ is the string value of in-order traversed subtree $t \in T'$ and $w_{t, T}$ is a weight given to a single subtree $t$ inside abstract syntax tree $T$. Fu \etal normalize the source code by transforming every variable name, array size definition and indexing of an array into single unified symbol. Then, authors remove all leaf nodes with common symbols to reduce noise, for example round and curly brackets. The similarity score between two source codes is calculated by normalizing the kernel values, leading ultimately to equation \ref{norm_kern_eqn}, which is the equivalent to cosine similarity \cite{Fu2017WASTKAW}. The last tree-based study is by Ganguly and Jones \cite{AIR2015}. They use information retrieval approach and treat every document as a \emph{pseudo-query}. This means that every document is first parsed into abstract syntax tree, then nodes belonging to a similar functionality are collected together. Finally, specific fields are gathered from this collection by using ranking scores. For example all class definitions are treated as one collection and from that collection, names of the classes are extracted as weighted terms to construct the pseudo-query. Ganguly and Jones claims that their approach allows to differentiate usage of same string literals in different situations.

Study utilizing an ensemble of methods by Xiong \etal presents their system named \textit{BUAA AntiPlagiarism} which uses abstract syntax tree to generate $n$-grams \cite{BUAA2009}. They first run the code through optimizer that gets rid of unnecessary complexity, then turn the simplified code into AST-representation and prune the tree by for example removing variable names and constants. This pruned tree is lastly travelled in preorder fashion that turns the tree into string format and forms $n$-grams from that representation. To calculate similarity between documents, Xiong \etal uses Jaccard coefficient, which was defined earlier in equation \ref{jacc_eqn}. Muddu \etal continues on combining approaches and presents their system called Code Plagiarism Detection Platform (CPDP) \cite{CPDPPD2013}. CPDP detects plagiarism by first tokenizing the AST, then turning the generated token stream into $4$-grams to be used in querying the matching documents. Finding the most closest document happens by using string matching algorithm \emph{Karp-Rabin Greedy String Tiling} given $n$-grams from the set of matching documents. Finally, the last similarity detection study is from Ganguly \etal in \cite{RCISCP2017}. They also use information retrieval approach in similar fashion as they did in previous work \cite{AIR2015}, to tackle with the problem related to $n$-grams without AST-representation; false-positives and exhaustive pair-wise calculation. Their method consists of building a pseudo-query and a ranked list of most matching documents. This pseudo-query is built by first retrieving three kinds of features from documents: lexical (3-grams); structural like identifiers, function types, and data types; and 11 stylistic features like average term length. Then documents are ranked according to term selection function for retrieving top ranked most similar documents. 

\subsubsection{Authorship identification}
The problem of authorship identification is very different from similarity detection and given in chapter \ref{chap-ai}. Instead of trying to find a function to represent a numerical value as similarity between two source code to detect plagiarism, authorship identification aims to reveal the writer of a document. It's common in following studies that the identification happens in closed environment, implying that the author of every document is known beforehand and can only be someone from the predefined set of possible authors. It's thus common that authorship identification can be used as authentication, and methods can be evaluated as the ground truth is known. Upcoming 15 studies reflect these findings.


Ding's and Samadzadeh's study follows the typical method of attribute counting studies. Authors extract total of 56 metrics belonging to three classes \cite{EJPFSAI2004}: layout, style and structure. Their feature selection is done by using variance and correlation analysis, whereas classification is done with \emph{canonical discriminant analysis}. Lange and Mancoridis extract 18 mostly text-based metrics and use genetic algorithm to find out the best combination \cite{UCMHGAAI2007}. Their classification is done by constructing a histogram per feature for every developer and then calculating which of the histograms are most closest to the unknown source code. Kothari \etal uses very similar histogram-based technique but considers style metrics and character distributions, namely character level $4$-grams \cite{APASCAI2007}. To select the most matching features for a single author, Kothari \etal uses information entropy which uses the distributions to make probabilistic evaluations. To classify the author, their approach is to have a database of writer profiles, extract metrics from source code and calculate the likelihood which known writer is the author. Arabyarmohamady \etal uses programming style to identify an author \cite{ACSBPD2012}. They build a profile for every author by transforming the source code into a feature vector \ie fingerprint and compare it to database of profiles to choose the most closest author profile. Plagiarism clusters are created by comparing the similarity of each feature vector with \emph{Euclidean distance}, thus allowing the detect issues with authorships and reveal plagiarism cases. Bandara and Wijayarathna has nine metrics that they use to generate tokens and token frequencies \cite{SCAIUFL2013}. For example, one of their metrics is number of characters per line (LLC) and to tokenize it, one creates token for specific length $n$ (LLC$_n$) and calculates the frequencies. This distribution of tokens is input to learning process called \emph{sparse auto-encoder} that learns to encode the features with neural network. Weights of this neural network are used as features to \emph{logistic regression} which classifies the author to document. Finally, similar study by again Bandara and Wijayarathna, uses now full neural networks for the same task \cite{DNNSCAI2013}. They use the same nine metrics with tokenization to get distributions per metric, and use them as a input to their deep neural network to learn to predict author from features.

Authorship identification with $n$-grams mostly use a baseline method called \emph{The Source Code Author Profile (SCAP)} in our review \cite{ESHPFSCAC2008, TSUDIJSCAI2011, CAPSCAP2014, ABEC2014}. The idea of SCAP is following: all known source codes from author $a$ are concated into one text file, $n$-grams are generated and only $L$ most frequent are kept per author to generate a profile $P$. To predict the author $\hat{y}$ of a source code $d$, one calculates how many $n$-grams does unknown profile $P_d$ has in common with pre-existing author profile $P_a$, or respectively

\begin{equation}
    \hat{y} = \argmax_{a \in A} |P_a \cap P_d|
\end{equation}

\noindent
The first study that uses its own method is by Burrows and Tahaghoghi. They approach the problem with information retrieval technique and consider author and document as queries \cite{SCANG2007}. Normalizing the documents is done by keeping only operators and keywords, while $n$-grams are used to present one document as overlapping sequences. Ranking the documents to create ranked list, happens with a proposed measure called \emph{Author1}. Author1-measure scores the similarity between documents and a query, and is defined using term frequencies for both query $q$ and document $d$

\begin{equation}
    \text{Author1}(q, d) = \sum\limits_{t \in q \cup d} \dfrac{1}{\min(|tf(t, q) - tf(t, d)|, 0.5)}
\end{equation}

\noindent
Burrow \etal continues on the topic of information retrieval in another study, where they experiment on six additional features on top of 6-grams \cite{AIRTSCAA2009}: white space, operators, literals, keywords, input/output (I/O) and function names. Rest of the $n$-gram related studies lean towards the SCAP method, mostly using it as a baseline while trying to improve it. For example Frantzeskou \etal analyzes the contribution of four different high-level features when using SCAP \cite{ESHPFSCAC2008}. These features are comments, layout features like spacing, identifier names and keywords. In another study, Frantzeskou \etal continues to use SCAP, but now studying the significance of user-defined identifiers with four categories \cite{TSUDIJSCAI2011}: identifiers using basic data types like \texttt{int} for integers, class identifiers, method identifiers and using all identifiers defined by the author. Tennyson and Mitropoulos study first the best profile length $L$ for SCAP \cite{CAPSCAP2014} and in another study, use two Bayesian methods to build an ensemble \cite{ABEC2014}. This ensemble works by using SCAP and Burrows method as a baseline to decide the author of a document. If there exists disagreement between baseline models, probability theory is used to classify the author. These two Bayesian methods are \emph{Maximum a Posteriori} and \emph{Bayes Optimal Classifier}, and both of them calculate probability that author $a$ wrote the document $d$ given data about authors previous work.

Alsulami \etal utilize deep neural network which uses features derived from the abstract syntax tree of a source code \cite{SCAANN2017}. Their method relies on learning the features from AST, rather than explicitly handcraft them. To learn these features, Alsulami \etal encode the tree as a vector by first traversing its nodes and subtrees with \emph{depth-first search}, then map them as a multidimensional vector called \emph{embedding layer} which works as an input. Lastly, the author classification is done using deep neural network. 

Finally, two hybrid studies are by Wisse and Veenman \cite{SDNAIJSP2015} and Zhang \etal \cite{AISC2017}. Wisse and Veenman approach the problem of authorship by using a features extracted directly from the AST. They first parse the source code into AST, then traverse it to derive metrics belonging to three classes: structural, style and layout. Structural features include most frequent $n$-grams, style features statistics about comments and layout features are various spacing related metrics. The classification is done by deriving a high dimensional feature vector and using \emph{Support Vector Machine}. Zhang \etal on the other hand, extract multiple features belonging to four classes: layout, style, structure and logic. In their study, layout features capture the usage of whitespace characters, style captures usage of variable names and lengths, structure statistics about methods and logic is defined as word-level $n$-grams.

% derived classification
\subsubsection{Findings}
Even though there exists multiple different ways to obtain similarity score between a pair of source codes, there are some reoccurring strategies for comparison: Jaccard similarity coefficient can be used for a similarity measure between two sets of tokens \cite{PACASCD2005, BUAA2009, OTIOLSS2015, ramirez2015high}, string edit distance is a simple but requires exhaustive pair-wise search to find direct occurrences \cite{LICD2010, ASTMLPD2013, CPDPPD2013}, cosine similarity can be used as similarity measure with vector space models often utilizing weighting schemes like term-frequency or term frequency–inverse document frequency \cite{AASCPD2012, USCR2014, Heblikar2015NormalizationBS, OTIOLSS2015, ramirez2015high} and similarity between tree structures can be calculated with a tree kernel as a dot product or by exhaustively comparing the nodes \cite{AAPSCDPTK2013, Fu2017WASTKAW, TBCFPD2012}. To reduce noise for similarity calculation, two kinds of approaches are used: preprocessing and normalization. In preprocessing the data is turned into another format \eg from plain text to AST or filtered to remove unnecessary information, and in normalization some weighting scheme is often applied or keyword generalization.

Author identification mainly uses metrics and $n$-grams to answer the question \emph{who wrote this code?}. Used metrics often belongs to three categories: layout, style and structure. However, coming up with meaningful metrics can be hard and can easily lead to feature engineering \cite{EJPFSAI2004}, where one needs to find a subset of well-performing features. $N$-grams are popular and used in SCAP, with information retrieval and alongside statistical metrics. The actual classification is in many cases done by representing source code as a vector of numerical values \ie the metrics, and then using supervised machine learning algorithm like the support vector machine \cite{SDNAIJSP2015}. 
