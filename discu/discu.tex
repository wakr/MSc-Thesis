In this chapter we first answer to our research questions, then discuss about limitations of our study and finally present ideas for the future work.

We asked following three research questions related to a more bigger question \emph{How plagiarism can be automatically detected?}.

\begin{itemize}
    \item[Q1:] \emph{What kind of approaches exist to detect source code plagiarism?}
    \item[Q2:] \emph{What are the possible benefits of using code structure for plagiarism detection?}
    \item[Q3:] \emph{How one can reduce the amount of false accusations?}
\end{itemize}

\paragraph{What kind of approaches exist to detect source code plagiarism?}
One must first define the term \emph{plagiarism} to be able to detect it. For example we defined it as stealing and passing ideas as one's own which closely relates copying other students work \ie having similar documents. In Chapter \ref{chap-liter-review}, we made the first division first between a similarity detection and authorship identification methods based on the literature survey. Using similarity detection, one is able to form a value between two documents that tells how similar these documents are, whereas using authorship identification, one can verify the author in theory. We found five subcategories from both methods: attribute counting, segment matching, $n$-grams, tree-based and hybrid. Approaches related to attribute counting forms frequencies or metrics from the source code, and some examples of metrics are number of lines, number of operators and number of tabs. Segment matching appears more frequently in similarity detection, where similarity value can be formed as easily as by calculating how many substrings two documents have in common. $N$-grams are by default free from the used programming language in documents, but easily grow the vocabulary size which creates noise to the observed data. Tree-based approaches transforms the source code into a tree format to get additional features. Lastly, hybrid approaches combine previously mentioned ideas to create more complex models. 

All previously mentioned subcategories handle the source codes in slightly different ways and many of the studies we encountered during the literature survey had different level of granularity. They all however, share the same two principles: source code documents have to be normalized in order to remove unnecessary features and one should always use a human expert to evaluate the final results. Using a human expert, the automatic detection is not in practise about directly detecting plagiarism, but more about retrieving interesting documents which stands out from the mass. Retrieving documents based on a given criteria resembles closely the functionality of a search engine and therefore the same theoretical principles can be applied, which was seen in many studies during literature review. 

The domain of plagiarism detection offers many use cases for machine learning and data exploration models because documents by themselves contain a lot of data hidden inside the actual written text. There is also a need for document suggestions so human experts don't have to go through everything manually. In our case, one batch of submissions contained around 200 files totaling 19\,900 unique pairs that needs to be compared. If it takes around one minute to for a human expert to say if a pair contains too much similarity, it would take approximately 13 whole days to come up with the result by using a single human expert. Therefore any approach that minimizes this time and has a good precision is worth of using.

\paragraph{What are the possible benefits of using code structure for plagiarism detection?}

The structure offers more data that what can be extracted solely from the 
written text. It also allows to generalize the source code, so it is able to show more high-level information, which can doable by generating the abstract syntax tree. However, even more important benefits are the ability to reduce the noise, to generalize the documents and make the detection process more stable. For example, we had use the structure for similarity detection in order to battle against obfuscation strategies like variable renaming or changing the order of expressions. By using the structure one is therefore able to 1) gain new features for the detection process, 2) restrict the variance of observed data, 3) make the detection resilient against common obfuscation strategies and 4) turn the focus more into the logic of the source code.



\paragraph{How one can reduce the amount of false accusations?}
Our hypothesis was that a model using the results of similarity detection and authorship identification would be able to reduce the number of false positives \ie authors who are being wrongly accused. However, by using Na√Øve Bayes with $n$-grams, a probabilistic machine learning model, we couldn't get results decent enough to be even consider using it to verify the detected authors. The size of our author pool was during the evaluation around 50 students with nearly 100 documents for each as a training data, which is nearly double than other studies in Chapter \ref{chap-liter-review}. Also, most of those studies had a pool under 10 authors. 

There are several reasons why did our model fail. First of all our vocabulary size was large and feature selection methods could have been utilized. Having a large vocabulary during authorship identification diminished the importance of possible features we hoped the model would capture. Second is the fact that both OHPE and OHJA guide the programming into a specific direction where
students learn a unified style to program these tasks. Having a general style learned from the course material and exercises, makes it impossible to detect authors using only the submissions and not the process. Finally, the documents are very similar naturally as the exercises measure a specific knowledge per task, meaning that the solution space will be very limited and no individual styles can emerge.

Something can be still said about reducing false accusations. Our similarity detection uses a parameter to control the search range, which reflects the threshold that decided when two source code documents are too similar. With a low threshold, the amount of false accusations grows quickly and vice versa \ie in order to minimize false positives, one can use a high threshold value. However as we saw during the evaluation, choosing the best $n$-gram length and threshold value can be difficult, as these values are highly data dependent. For example the submissions to SOCO competition differs a lot from our data set and the only the training set of it is human evaluated. Despite these shortcomings, SOCO's training set is one of the few publicly available data sets, that offers a plagiarized documents which are all prelabeled and not synthetic. JPlag, on the other hand, is a existing tool for plagiarism detection and as we saw during our evaluation, even it suffers from false positives. The people using it must also come up with a way of deciding a working threshold value, which in many cases can be impossible due the lack of proper domain knowledge. Another thing that introduces false positives, are the submissions for the same exercise, where the solution space for is just too small to generate varying solutions. In the latter case, detecting plagiarism can be practically impossible by just using the information about the submitted source code.
\mbox{}\\

Our study has limitations, mostly due to the assumptions we made. These are 1) in-class plagiarism, 2) exercise focus, 3) single author and 4) plagiarism direction. As these assumptions mostly restrict the plagiarism to exist inside a single course and exercise, we can't say it's very realistic situation because plagiarism can also be ongoing and exist between various courses by same plagiarists. For example a student could use answers from previous iterations of the course, she could use material found from the Internet, or she could use a friend who has already completed the course. These cases are difficult to detect as most of the information can only be found outside the course, and thus many academic studies use same assumptions as we, only difference being detecting the plagiarism direction. There are many cases however, where the direction is important for example when one wants to find the original source \ie the sharer. Another major limitation is, as pointed out before, the poor performance of authorship identification. With our real life data sets, the amount of false positives in OHPE's third task was so much that we had to limit it by filtering out detected papers and our strategy was to gather only papers for the biggest cluster emerged in OHPE, where the similarity was maximum. Because we had to resort to this action in order to get a result from our human expert, there can exist many true positives our approach couldn't caught. 

Another group of limitations are based on the choice of our approach, the most problematic being the choice of the training data set for similarity detection. We used the full training set of SOCO competition and its two test sets, one containing no plagiarism and the another 28 cases decided by a majority voting of other models, to tune our similarity detection model. Our results show that the choice for hyperparameters using another non-course-based data set, is not easy to make as there exist a lack of proper domain knowledge. For example 0.4 as the $\varepsilon$-range could work well for SOCO, but produce large amount of false positives in OHPE. Other similarity detection related limitation is that we use a fixed similarity measure and didn't try out other measures like for example the Euclidean distance. On the other hand, limitations related to authorship identification include two major topics: we didn't restrict the vocabulary size using feature selection strategies and we used a relatively simple probabilistic model. The vocabulary size for us was 278\, 000 unique features using character-level $n$-grams, which could have been minimized by applying statistical methods \eg chi-squared test, to select only a set of most important features for classification. When speaking of the model we used, it is a simple probabilistic model which is often used as a baseline model and the assumption about non-existing feature correlation is not true in real life, because source code is written by following the grammar rules and various stylistic preferences \eg spacing. 

For the future, there are many approaches which could be taken. As an example, one could use the process contrast to the final product, to get more in-depth analysis of how the plagiarism occurs. One could also track the style of an author inside a time frame, to see if it remains same over time or changes. Different normalization methods and tokenization for both similarity detection and authorship identification should be used, but at the moment it is difficult as every source code data set differs from each other thus no general guidelines can be given. Another important aspect is also how the source code files are represented and handled during the detection. We used the tree-format and project-level detection, but as well found from other studies varying representations and granularities. For example to minimize the amount of false positives during similarity detection, one could first decompose a source code document into lines or other more coarse units and then run the detection so that only parts of the documents would be under the detection. But, even this approach wouldn't help if the tasks are too restricted by nature. Our results also revealed clusters which could be studies more and generally more information about possible plagiarism can be used to enhance the detection, like are same students regularly inside same clusters and how correct non-plagiarized solutions could be filtered out from the process. These emerging false positives are such a large issue, that more effort should be put into tackling with them and looking if it's possible at all to find plagiarism inside a single course with short exercises. 

We suggest that the concept of source code plagiarism should be more researched, which means creating more studies answering to questions like 1) \emph{What are the other topics related to source code plagiarism detection than similarity detection and authorship identification?}, 2) \emph{What is the amount of data one needs to have for a precise detection?}, 3) \emph{What kind of authorship identification is possible to apply in a course with hundreds of students?}, 4) \emph{How much and what type of normalization can reduce the noise enough to have a precise model?} and 5) \emph{What types of data can be collected from a student, so she can be accused from the act of plagiarism?}. The field of source code plagiarism detection offers many use cases for techniques like large-scale machine learning, statistical modeling, data mining, data visualization and artificial neural network, but the problem lies more in defining the actual plagiarism. Even our human expert told after the evaluation, that it was hard for some tasks to tell if it was plagiarism or just coincidental similarity, which raises the question that how much evidence the plagiarism detection process should gather and from where. We believe that using one course as a detection target makes the overall detection difficult and using only one model, makes accurate detection process nearly impossible. Thus more effort should but into research of multi-model approaches, where similarity detection plays a small part of much bigger detection process. 

