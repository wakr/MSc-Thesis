\subsection{Authorship identification}

The second method we apply, is the author identification from a collection of source codes. Like in similarity detection, we apply this to one exercise at a time but as this model requires training, we define the training set to be all documents that the author has previously written. For example when considering a course which consist of seven weeks and the final exam, we can use all seven weeks per author to train the model \ie try to capture the preferred style of an author, and then predict for a random sample from a collection of exam submission that who is the most likely author.  

The algorithm for authorship identification we use is heavily based on the probabilistic model Naïve Bayes from Chapter \ref{chap-bg-classification}. We utilize $n$-grams which was a popular method among other studies in Chapter \ref{chap-liter-review-methods}, because it captures preferences that the author might have when writing a program by using character-level information. That is also why we don't apply a lot of normalization for authorship identification, as this information would be lost if too much transformation would be applied. The pseudocode for our authorship identification is seen in Algorithm \ref{alg-ai}.

% multi label?

\begin{algorithm}[ht]
\caption{Detecting author candidates for a source code.}
\label{alg-ai}
\begin{algorithmic}

\Require Set of authors $A$
\Require Set of documents $D$ belonging to authors $A$
\Require Index of the exercise under detection $i \in \mathbb{N}$
\Require Length of character level $n$-grams $n \in \mathbb{N}$
\Procedure{TrainAndPredictAuthor}{$A, D, i, n$}
   \State $\bolditt{X} \gets$ \Call{ExctractNgrams}{$D, n$}
   \State $\bolditt{W} \gets$ \Call{TFIDF}{$\bolditt{X}$}
   \State $\bolditt{W}_{train}, \bolditt{y}_{train}, \bolditt{W}_{test}, \bolditt{y}_{test} \gets$ \Call{Split}{$\bolditt{W}, A, i$}
   \State $NB \gets$ \Call{TrainNaïveBayes}{$\bolditt{W}_{train}, \bolditt{y}_{train}$}
   \State $A_{auth} \gets$ \Call{Predict}{$NB, \bolditt{W}_{test}$}
   \State \textbf{return} $A_{auth}$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\noindent
The authorship identification in Algorithm \ref{alg-ai} is dependent from one hyperparameter, which is the length of character level $n$-grams. The value for it will be tuned using both data sets OHPE and OHJA, and choosing the value which performs best on average. 

The remaining flow of Algorithm \ref{alg-ai} is following. After the weight matrix $\bolditt{W}$ has been formed we split the data into training and test sets with appropriate classes $\bolditt{y}$, which indicates the authorship assignments. The split is done by treating the $i$th exercise as a test set, and everything before it as a training data. For example if the interest is the exam, which can be thought as the final task of the course, there are 135 exercises before it in OHPE and 79 in OHJA\footnote{Values 135 and 79 are after pair programming tasks are filtered out from both sets.} that can be used to capture the individual style of an author. The appropriate training data is given to the Naïve Bayes algorithm in \textproc{TrainNaïveBayes}, which theoretical background is given in Chapter \ref{chap-bg-classification}. The training of the Naïve Bayes algorithm allows therefore to estimate the probabilistic parameters inside the model, and the function \textproc{Predict} being the maximum a posteriori probability (MAP) estimate, is able make the author prediction.

% ohpe exam 4 parts, 135 tasks before, removed pair programming
% ohja exam 3 parts, 79 tasks before, removed pair and course feedback



