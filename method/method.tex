
In this thesis, we explore the possibility of using a two-phase model that combines both similarity detection and authorship identification, and believe that such model increases the precision of the detection process. It is noticeable that none of the studies that were part of the literature review in Chapter \ref{chap-liter-review} combines these two approaches when making the final decision, even though methods that are being used share a common target; to detect plagiarism. 

Thus we base our model to a two phase model which could reduce the amount of false-positives found in source code plagiarism detection. False-positives are problematic as it means an innocent author is considered to be a possible plagiarist, and therefore having too sensitive model introduces a lot of extra work. Two phases of our model are similarity detection and authorship identification, where both of them are able to define a set of authors; similarity detection reveals suspicious authors based on the file similarity and authorship identification predicts candidate authors for a document $d$. 

%pre-process documents, tokenize documents, exclude templates, calculate similarities and finding suspects using these similarity scores.

Both of our models are inspired by other studies presented in the literature review and combine the high-level approach used in many tools \cite{RSCAD2016}: preprocess, normalize, evaluate and predict. For similarity detection we use lower level features that capture the structure and are resilient against transformations introduced in Chapter \ref{chap-liter-review-methods}, and for authorship identification we use higher level features which can capture the style of an author. The generalization of the proposed model is given below.

\begin{algorithm}[ht]
\caption{Detecting plagiarism between a set of source code files.}
\label{alg-toplvl}
\begin{algorithmic}

\Require Set of authors $A$
\Require Set of source code files $D$ written by various authors $\forall a \in A$
\Require Index of the exercise of interest $i$
\Require Length of word level $n$-grams $n_w \in \mathbb{N}$
\Require Length of character level $n$-grams $n_c \in \mathbb{N}$
\Require Minimum rate of similarity $\varepsilon \in [0, 1]$
\Procedure{PLGdetect}{$A, D, i, n_w, n_c, \varepsilon$}
   \State $D'\gets$ \Call{normalize}{$D$}
   \State $A_{susp} \gets$ \Call{detectSim}{$A$, $D'_i, n_w, \varepsilon$}
   \State $A_{auth} \gets$ \Call{trainAndPredictNB}{$A, D', i, n_c$}
   \State \textbf{return} $A_{auth} \cap A_{susp}$
\EndProcedure

\end{algorithmic}
\end{algorithm}

\noindent
In Algorithm \ref{alg-toplvl}, source code files are first normalized for similarity and author detection separately \ie different strategies are used for both. Then similarity is detected for a collection of documents belonging under same exercise with the function \textproc{DetectSim}, which ultimately forms a group of suspicious authors noted as $A_susp$. The process is controlled by the parameter $\varepsilon$ which acts as a threshold for the detection. For example $\varepsilon = 1.0$ means that documents must be exact copies in order to group them together. Another function is the \textproc{trainAndPredictNB} that trains the model with previous documents that the author has written, and then predicts who are the most likely authors from the set $A$ for exercise number $i$ noted as $A_{auth}$. Finally, intersection between candidate authors and suspicious authors is taken, to provide a set of possible plagiarists. 

Both parts of our model are validated against real-life tools; plagiarism detection is evaluated against JPlag \cite{prechelt2002finding} which has been used as a baseline model in a SOCO competition \cite{saez2014pan}, and authorship identification against SCAP method which was found to be commonly used by other studies in Chapter \ref{chap-liter-review-methods}.

\subsection{Assumptions}

We mainly focus on academia and especially to programming courses that are offered by 
universities. Following five assumptions are defined to simplify the problem of plagiarism
detection by allowing us to concern only plagiarism that happens in a closed environment and within a closed set of documents. 

\paragraph{In-class plagiarism} Plagiarism has occured only inside a 
specific course implementation. Let $\mathcal{P}(A)$ be a powerset of students within offered courses in university. We are only interested about a set of students referred as authors $A$ attended in a specific course $c$ \ie a subset $A_c \subseteq \mathcal{P}(A), A_c \neq \emptyset$. The corpus $D_c$ is built by gathering every submission done by students $\forall a \in A_c$ and a set of documents belonging to individual student is defined as $D_a = \{d \mid d \in D_c, a = auth(d)\}$. 


\paragraph{Exercise focus} 
Let $E_c = \{e_1, e_2, ..., e_n\}$ be a set of exercises for a course $c$, then submissions for a single exercise is represented by a subset $D_{c,e} \subseteq D_c$. With this assumption, we focus the plagiarism detection to submissions done to a single exercise at a time \ie plagiarism can happen only between submissions to a single exercise.

\paragraph{Single author} 
Every source code $d \in D_c$ is assumed to have a single author $a = auth(d), a \in A_c$. This allows us to assume that every source code submissions is done as a individual work, and all results that suggests otherwise implies about the case of excessive collaboration \ie plagiarism. 

\paragraph{Plagiarism direction} 
Let a file $d_i$ be plagiarized from $d_j$ \ie $d_i \xrightarrow{plag} d_j$, we treat this as same as the opposite direction $d_i \xleftarrow{plag} d_j$, making the direction of plagiarism unimportant. This means that we treat both cases; sharing and copying as an act of supporting plagiarism. 

\paragraph{Expert interference}
We believe that no system can be accurate enough to autonomously accuse students about plagiarism. However, this is doable when some form of human judgment is added to the model. In principal this means that the model can make predictions about cases of plagiarisms which we call \emph{suspects}, but the human expert must make the \emph{allegation} of plagiarism based on the results and after questioning the students. Every university should however have a guideline about what is considered as plagiarism and how such cases are handled, like the University of Helsinki has done\footnote{\url{https://blogs.helsinki.fi/alakopsaa/?lang=en} Accessed 9th May 2018}.

\input{method/data.tex}


% pariohjelmointi?

%Consider for example three programs presented in appendix \ref{appendix:programs}. Because the solution to the given task is very limited, the submissions will inevitably have similar solutions, questioning the very fact that can there even exist plagiarism in short tasks. To overcome this problem we filter out all tasks where average line count falls to first quartile \ie under the 25th percentile, which is 21.0 for OHPE and 54.5 for OHJA. We call these filtered out tasks as \emph{trivial tasks} \ie exercises which solution space is very restricted. The intuition behind our filtering is that one cannot simply find plagiarism from too simple exercises which includes given tasks like: printing \say{Hello World} (1st task of OHPE), calculating how many seconds are in a year (6th task of OHPE) or doing simple string concatenation (2nd task of OHJA). 

\subsection{Document normalization}

We utilize same approaches as studies reviewed in Chapter \ref{chap-liter-review-methods} to minimize the variance between documents. The benefit of normalization has been, that it reduces the noise and allows to capture only the information which is crucial. In case of similarity detection the structure information must be preserved, and in case of similarity detection the stylistic preferences must be captured.

For similarity detection we transform every document into a token stream by first parsing the program with a parser and turning it into abstract syntax tree, then traversing the structure to get the stream as a string format. This method allows to capture the higher-level structure of the program, and still allows to handle it as a text. Also, it works against obfuscation strategies which were stated in Tables \ref{tbl-plag-strat}, \ref{tbl-plag-transf} in Chapter \ref{chap-bg-sc-plag}, by ignoring certain aspects. For example the parses will ignore all white spaces, comments, identifier names and standardizes loop names, working against levels 1,2 and 5 of Table \ref{tbl-plag-transf}. The parser itself only works with Java and is heavily inspired by the one used in JPlag \cite{prechelt2002finding} and the complete list of tokens is seen in Appendix \ref{appendix:token-list}, which shows also the equivalencies to generate certain tokens. For example all loop constructs generate a single token "LOOP\{" to indicate start of the loop. This normalizes the documents to preserve the underlying logic behind them, so even if the plagiarists would use a different looping construct like while instead of for, two programs would still share a common token stream. 

Table \ref{tbl-token-stream} shows the corresponding token stream for program A in Appendix \ref{appendix:programs}, where one can see how much information is discarded from the source code as we only keep the crucial structural information.

\begin{table}[ht]
\centering
\caption{Token stream generated from the example source code.}
\label{tbl-token-stream}
\begin{tabular}{|l|l|} \hline
\bf Original source code & \bf Token stream \\ \hline
\begin{lstlisting}
public class A{

     public static void main(String[] args){
        int a = 5;
        int b = 10;
        int c = 2;
        double d = (a + b + c)/(double)3;
        System.out.println(d);
     }
}
\end{lstlisting}                     &

\begin{lstlisting}
CLASS{  
VOID    
METHOD{ 
VARDEF 
ASSIGN  
VARDEF 
ASSIGN  
VARDEF 
ASSIGN  
VARDEF 
ASSIGN  
APPLY   
}METHOD 
}CLASS
\end{lstlisting}
\\ \hline      
\end{tabular}
\end{table}

For the authorship identification, normalization method we apply uses the same idea as in \cite{AIRTSCAA2009, SCANG2007}. We discard all comments and normalize literal values to remove any possible notion of the original author, like unique student number or name in comments. The reason behind normalization for authorship identification is therefore to leave the original document as intact as possible, maintaining the preferences that the programmer might have for \eg variable naming or spacing. An example of the normalization procedure is given in Table \ref{tbl-ai-normalization} for the same program used in Table \ref{tbl-token-stream}, where one can see that all numerical values have been transformed under a single dollar token \texttt{\$}.

\begin{table}[ht]
\centering
\caption{The result of normalization procedure for authorship identification.}
\label{tbl-ai-normalization}
\begin{tabular}{|l|} \hline
\bf Normalized code \\ \hline
\begin{lstlisting}
public class A{

     public static void main(String[] args){
        int a = $;
        int b = $;
        int c = $;
        double d = (a + b + c)/(double)$; 
        System.out.println(d);
     }
}
\end{lstlisting} 

\\ \hline      
\end{tabular}
\end{table}

\subsection{Document representation}

To represent every documents as vector, we use information retrieval techniques introduced in Chapter \ref{chap-IR}. Plagiarism detection is therefore done first by converting document into vector space model after the normalization of documents. In both similarity detection and authorship identification documents terms are first extracted, which in our case means all possible $n$-grams with respect to vocabulary $\mathbb{V}$. The only difference being that in similarity detection the vocabulary is formed using every document as a token stream, where as authorship identification uses only part of the complete data to form the available vocabulary. 

To overcome the problem with varying document length and frequently appearing terms, we apply tf-idf weighting introduced in Chapter \ref{chap-IR-document-repr}. Table \ref{tbl-ngram-sd} shows example of the term extraction for similarity detection using word level 2-grams for program A in Appendix \ref{appendix:programs}. All tf-idf weights have been normalized using Euclidean norm 

\begin{equation}
    \dfrac{\bolditt{x}}
          {\sqrt{\sum \limits_i^{|\mathbb{V}|} x_i^2}}
\end{equation}

% show example calculation

\begin{table}[ht]
\centering
\caption{Similarity detection term extraction for document A. Terms are 2-grams extracted from token stream, tf-idf weights have been normalized and values rounded at two decimal places.}
\label{tbl-ngram-sd}
\begin{tabular}{l|c|c}
\bf Term & \bf Raw frequency & \bf TF-IDF weight \\ \hline
    \texttt{APPLY \}METHOD} & 1 & 0.14\\
    \texttt{ASSIGN APPLY} & 1 & 0.18\\
    \texttt{ASSIGN VARDEF} & 3 & 0.55\\
    \texttt{CLASS\{ VOID} & 1 & 0.14\\
    \texttt{METHOD\{ APPLY} & 0 & 0.00\\
    \texttt{METHOD\{ VARDEF} & 1 & 0.18\\
    \texttt{VARDEF ASSIGN} & 4 & 0.74\\
    \texttt{VOID METHOD\{} & 1 & 0.14\\
    \texttt{\}METHOD \}CLASS} & 1 & 0.14\\
\end{tabular}
\end{table}

\noindent
To get the value 0.18 for a term \texttt{ASSIGN APPLY} in document A one sees first that the value of $tf$ is 1 from Table \ref{tbl-ngram-sd}. The $idf$ is formed by dividing number of documents with the number of total term frequency over all documents, and taking a logarithm \ie $idf = \log(N/df) = \log((1+3)/(1+2)) + 1 \approx 1.29$. Note that we add extra ones to avoid division with zero and to diminish the effect of terms appearing only in training set. Now $tf{\text -}idf$ is simply $tf \cdot idf = 1 \cdot 1.29 = 1.29$. Finally, after calculating non-normalized weight for each term, we can derive the value 0.18 dividing $1.29$ with the Euclidean norm over the weights which gives $tf{\text -}idf_{norm} = 1.29 / \! \norm{\bolditt{w}}_2 = 1.29 / 6.98 = 0.18$

The vocabulary $\mathbb{V}$ that forms the possible tokens in Table \ref{tbl-ngram-sd}, is the union between every token appearing in three example documents \ie $\mathbb{V} = \bigcup_{i=1}^{3} V_i$ where $V_i = \{t_1, t_2, \cdots, t_n\}$. Therefore some terms may appear zero like the term \texttt{METHOD\{ APPLY} for document A in Table \ref{tbl-ngram-sd}, as it exists only in the token stream of document C. However, this problem with terms appearing zero times is only present in authorship identification, where the main corpus is the training set that is needed to train the model. Therefore even though there can be some terms that appears only in training set and not in test set, the smoothing we apply also in Equation \ref{eq-laplace} prevents the complete product to become zero. Terms like \texttt{ASSIGN VARDEF} and \texttt{VARDEF ASSIGN} have a high weight as they mostly appear in document A, implying that document A has more variable assignments than document B or C, which is true when one looks at the raw source code documents. 

With our approach, we can now represent document as a vector of weights \eg document A as $\bolditt{x} = [0.14, 0.18, \cdots, 0.14, 0.14]$, and the dimension of $\bolditt{x}$ is the size of vocabulary $\mathbb{V}$. The visualization of these three programs as vectors of weights is seen in Figure \ref{fig-tfidf}, where it is clear that program C is the outlier whereas A and B share more similarities between each others.

\begin{figure}[!h]
\centering
\setlength\figureheight{7cm}
\setlength\figurewidth{10cm}
\input{plots/tfidf.tikz}

\caption{Three sample programs from Appendix \ref{appendix:programs} visualized in two dimensions. Tf-idf weights have been calculated from the token streams.} \label{fig-tfidf}
\end{figure}



\input{method/similarity_detection.tex}

\input{method/author_identification.tex}

%\begin{figure}[!h]
%\centering
%\setlength\figureheight{5cm}
%\setlength\figurewidth{8cm}
%\input{plots/fig.tikz}
%
%\caption{TEST} \label{fig:M1}
%\end{figure}

%use t-SNE for visualization!! density is lost https://stats.stackexchange.com/questions/263539/k-means-clustering-on-the-output-of-t-sne?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa


\subsection{Evaluation}

asd


\newpage